<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jefferyy-peng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jefferyy-peng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-09T06:43:09+00:00</updated><id>https://jefferyy-peng.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Generalization Through Variance in Diffusion Models</title><link href="https://jefferyy-peng.github.io/blog/2025/diffusion-generalization-variance/" rel="alternate" type="text/html" title="Generalization Through Variance in Diffusion Models"/><published>2025-05-21T16:40:16+00:00</published><updated>2025-05-21T16:40:16+00:00</updated><id>https://jefferyy-peng.github.io/blog/2025/diffusion-generalization-variance</id><content type="html" xml:base="https://jefferyy-peng.github.io/blog/2025/diffusion-generalization-variance/"><![CDATA[<p>”””</p> <h1 id="generalization-through-variance-in-diffusion-models--logical-summary">Generalization Through Variance in Diffusion Models — Logical Summary</h1> <h2 id="1-core-question-of-the-paper">1. Core Question of the Paper</h2> <p>The paper asks a precise and nonstandard question:</p> <blockquote> <p><strong>Why do diffusion models trained with denoising score matching (DSM) generate samples that go <em>beyond</em> memorizing the training data, even when the true score would only reproduce the training examples?</strong></p> </blockquote> <p>Equivalently:</p> <ul> <li>Why does training with the <em>proxy score</em> \(\tilde s(x,t;x_0)\) lead to <strong>generalization</strong>, whereas the <em>true score</em> \(s(x,t)\) would lead to exact memorization?</li> </ul> <p>The paper’s answer is:</p> <blockquote> <p><strong>Generalization arises from variance in the learned score, not from bias.</strong></p> </blockquote> <hr/> <h2 id="2-forward-diffusion-and-scores-setup">2. Forward Diffusion and Scores (Setup)</h2> <h3 id="forward-process">Forward process</h3> <p>Data points \(x_0 \sim p_{\text{data}}\) are corrupted by a linear SDE: \(\dot x_t = -\beta_t x_t + G_t \eta_t,\) leading to Gaussian conditionals: \(p(x \mid x_0, t) = \mathcal N(x; \alpha_t x_0, S_t).\)</p> <h3 id="scores">Scores</h3> <ul> <li> <p><strong>True score</strong>: \(s(x,t) := \nabla_x \log p(x \mid t).\)</p> </li> <li> <p><strong>Proxy score</strong> (DSM target): \(\tilde s(x,t;x_0) := \nabla_x \log p(x \mid x_0,t) = S_t^{-1}(\alpha_t x_0 - x).\)</p> </li> </ul> <p>Key identity: \(\mathbb E_{x_0 \mid x,t}[\tilde s(x,t;x_0)] = s(x,t).\)</p> <p>So the proxy score is <strong>unbiased</strong>, but <strong>noisy</strong>.</p> <hr/> <h2 id="3-dsm-objectives-and-the-key-observation">3. DSM Objectives and the Key Observation</h2> <p>Two losses:</p> <ul> <li>Ideal (unavailable): \(J_0 = \mathbb E\, \| \hat s - s \|^2\)</li> <li>Practical (DSM): \(J_1 = \mathbb E\, \| \hat s - \tilde s \|^2\)</li> </ul> <p>Although \(J_0\) and \(J_1\) share the same minimizer in infinite-capacity limits, <strong>models trained with \(J_1\) generalize better</strong>.</p> <p>The paper shows this is because the proxy score has <strong>nontrivial covariance</strong>: \(C(x,t) := \mathrm{Cov}_{x_0 \mid x,t}[\tilde s(x,t;x_0)] = S_t^{-1} + \nabla_x^2 \log p(x \mid t).\)</p> <p>This covariance is:</p> <ul> <li>large at small \(t\) (since \(S_t \to 0\)),</li> <li>large near training points and between them (high curvature regions).</li> </ul> <hr/> <h2 id="4-sampling-via-pf-ode-and-the-central-difficulty">4. Sampling via PF-ODE and the Central Difficulty</h2> <p>Sampling uses the <strong>probability flow ODE (PF-ODE)</strong>: \(\dot x_t = -\beta_t x_t - D_t \hat s_\theta(x_t,t).\)</p> <p>For fixed \(\theta\) and \(x_T\), this is <strong>deterministic</strong>: \(x_0 = F(x_T, \theta).\)</p> <p>But:</p> <ul> <li>\(\hat s_\theta\) depends on random training samples,</li> <li>hence \(F(x_T,\theta)\) is a <strong>random variable</strong>.</li> </ul> <p>We want the <strong>distribution</strong> of outputs, not their mean: \(\bar q(x_0 \mid x_T) = \mathbb E_\theta[\delta(x_0 - F(x_T,\theta))].\)</p> <p>This delta form is <strong>not optional</strong> — it is the definition of the distribution induced by a random variable.</p> <hr/> <h2 id="5-path-integral-reformulation-key-technical-contribution">5. Path Integral Reformulation (Key Technical Contribution)</h2> <p>Directly averaging \(F(x_T,\theta)\) is intractable because:</p> <ul> <li>PF-ODE has no closed-form solution,</li> <li>\(\hat s_\theta\) enters nonlinearly through the trajectory.</li> </ul> <p><strong>Key trick</strong>: Rewrite PF-ODE sampling as a <strong>path integral</strong> enforcing the ODE as a constraint: \(q(x_0 \mid x_T; \theta) = \int \mathcal D[x_t]\mathcal D[p_t]\, \exp\left( \int_T^\epsilon i p_t \cdot [\dot x_t + \beta_t x_t + D_t \hat s_\theta(x_t,t)]\,dt \right).\)</p> <ul> <li>The auxiliary field \(p_t\) enforces the ODE via a delta-functional.</li> <li>Crucially, \(\hat s_\theta\) appears <strong>linearly inside the exponent</strong>.</li> </ul> <p>This transforms the problem into averaging an <strong>exponential of a linear functional of a random function</strong>.</p> <hr/> <h2 id="6-averaging-over-training-randomness">6. Averaging Over Training Randomness</h2> <p>Because of linearity, the ensemble average becomes a <strong>cumulant expansion</strong>: \([ q(x_0 \mid x_T) ] = \int \mathcal D[x_t]\mathcal D[p_t]\, \exp\left( M_1 - \tfrac12 M_2 + \cdots \right).\)</p> <p>Where:</p> <ul> <li>\(M_1\) depends on the <strong>mean score</strong> \(s_{\text{avg}}(x,t) := [\hat s_\theta(x,t)].\)</li> <li>\(M_2\) depends on the <strong>score covariance</strong> \(V(x,t;x',t') := D_t\,\mathrm{Cov}_\theta[\hat s(x,t),\hat s(x',t')]\,D_{t'}.\)</li> </ul> <p>Assuming higher cumulants are negligible (Gaussian approximation), the averaged dynamics are equivalent to an SDE.</p> <hr/> <h2 id="7-main-result-effective-sde-for-the-typical-learned-model">7. Main Result: Effective SDE for the Typical Learned Model</h2> <p><strong>Proposition (central result)</strong>: Sampling from \([q(x_0 \mid x_T)]\) is equivalent to integrating: \(\dot x_t = -\beta_t x_t - D_t s_{\text{avg}}(x_t,t) + \xi(x_t,t),\)</p> <p>where: \(\mathbb E[\xi] = 0, \qquad \mathbb E[\xi(t)\xi(t')] = V(x_t,t;x_{t'},t').\)</p> <p>Interpretation:</p> <ul> <li>The <strong>mean dynamics</strong> follow the deterministic PF-ODE.</li> <li><strong>Generalization arises entirely from the noise term</strong>.</li> </ul> <p>If \(V = 0\) → pure memorization. If \(V \neq 0\) → stochastic spreading → generalization.</p> <hr/> <h2 id="8-concrete-example-naive-score-estimator">8. Concrete Example: Naive Score Estimator</h2> <p>The paper shows this is not abstract theory by constructing a toy algorithm:</p> <p>At each Euler step:</p> <ol> <li>Sample \(x_{0t} \sim p(x_0 \mid x_t,t)\),</li> <li>Use a noisy score: \(\hat s(x_t,t) = s(x_t,t) + \sqrt{\kappa/\Delta t} \,[\tilde s(x_t,t;x_{0t}) - s(x_t,t)].\)</li> </ol> <p>Despite using the proxy score directly (which looks like memorization), the <strong>resampling-induced variance</strong> yields: \(V(x,t;x',t') = \kappa D_t C(x,t) D_{t'}\,\delta(t-t').\)</p> <p>Thus:</p> <ul> <li>Noise is strongest where proxy-score covariance \(C(x,t)\) is large,</li> <li>i.e. near boundaries between training examples.</li> </ul> <p>This proves:</p> <blockquote> <p><strong>Variance alone is sufficient to induce generalization.</strong></p> </blockquote> <hr/> <h2 id="9-final-interpretation">9. Final Interpretation</h2> <ul> <li>The effective PF-ODE: <ul> <li>follows deterministic score flow <strong>on average</strong>,</li> <li>most probable trajectories stay near deterministic paths,</li> <li>but structured noise spreads probability mass.</li> </ul> </li> </ul> <p>Therefore:</p> <ul> <li>Training examples remain dominant modes,</li> <li>but probability mass fills in between them.</li> </ul> <p>This explains <strong>why diffusion models generalize without forgetting</strong>.</p> <hr/> <h2 id="10-one-sentence-takeaway">10. One-Sentence Takeaway</h2> <blockquote> <p><strong>Diffusion models generalize because randomness in score estimation induces an effective stochastic dynamics whose noise is strongest exactly where the model is uncertain, causing probability mass to spread between training examples while preserving their dominance.</strong> “””</p> </blockquote>]]></content><author><name></name></author><category term="sample-posts"/><category term="Learning"/><summary type="html"><![CDATA[Paper summary for "Generalization through Variance in Diffusion Models"]]></summary></entry><entry><title type="html">How Diffusion Models Work</title><link href="https://jefferyy-peng.github.io/blog/2025/diffusion-model/" rel="alternate" type="text/html" title="How Diffusion Models Work"/><published>2025-03-19T16:40:16+00:00</published><updated>2025-03-19T16:40:16+00:00</updated><id>https://jefferyy-peng.github.io/blog/2025/diffusion-model</id><content type="html" xml:base="https://jefferyy-peng.github.io/blog/2025/diffusion-model/"><![CDATA[<p>As part of my research into the mechanisms of compositionality in generative AI, I am conducting a review of the foundational literature surrounding Diffusion Models. While these models have achieved ubiquity in text-to-image generation, understanding their underlying mathematical formulations and architectural constraints is essential for addressing their current limitations in factorization and generalization.</p> <p>This post summarizes the theoretical framework of diffusion models, covering the probabilistic definitions, conditioning mechanisms, and the trade-offs between pixel-space and latent-space architectures.</p> <hr/> <h2 id="21-formulations-of-diffusion-models">2.1 Formulations of Diffusion Models</h2> <p>Diffusion models can be formalized through three primary frameworks: <strong>Denoising Diffusion Probabilistic Models (DDPMs)</strong>, <strong>Score-based Generative Models (SGMs)</strong>, and <strong>Stochastic Differential Equations (SDEs)</strong>.</p> <h3 id="211-denoising-diffusion-probabilistic-models-ddpms">2.1.1 Denoising Diffusion Probabilistic Models (DDPMs)</h3> <p>The DDPM framework defines a forward Markov chain that gradually adds Gaussian noise to data \(x_0\) until it approaches an isotropic Gaussian distribution \(x_T\). The transition kernel is defined as: \(q(x_t \mid x_{t-1})=\mathcal{N}\!\left(x_t;\sqrt{1-\beta_t}\,x_{t-1},\beta_t I\right),\) where \(\beta_t \in (0,1)\) controls the noise variance.</p> <p>Let \(\alpha_t := 1-\beta_t\) and \(\bar{\alpha}_t := \prod_{s=1}^{t} \alpha_s\). Then the marginal distribution is:</p> \[q(x_t \mid x_0)=\mathcal{N}\!\left(x_t;\sqrt{\bar{\alpha}_t}\,x_0,(1-\bar{\alpha}_t)I\right).\] <p>Using the reparameterization trick with \(\epsilon \sim \mathcal{N}(0,I)\), we can write:</p> \[x_t=\sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon.\] <p>The generative capability arises from learning a parameterized reverse Markov chain. The model estimates the transition kernel \(p_\theta(x_{t-1}\mid x_t)\) to iteratively denoise the latent variables:</p> \[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\] <p>Training is typically performed by optimizing a variational lower bound (ELBO) on the negative log-likelihood.</p> <blockquote> <p><strong>Step 1</strong>: Marginalize likelihood: the probability of \(x_0\) is obtained by integrating out all latent variables \(x_{1:T}\). This integral is generally intractable.</p> \[\mathbb{E}\big[-\log p_\theta(x_0)\big] = \mathbb{E}\left[-\log \int p_\theta(x_{0:T}) \, dx_{1:T}\right]\] <p><strong>Step 2</strong>: Introduce an auxiliary (variational) distribution \(q(x_{1:T}\mid x_0)\) by multiplying and dividing inside the integral.</p> \[= \mathbb{E}\left[ -\log \int p_\theta(x_{0:T}) \frac{q(x_{0:T})}{q(x_{0:T})} \, dx_{1:T} \right]\] <p><strong>Step 3</strong>: Apply Jensen’s inequality \(-\log\) is convex, moving the log inside the expectation. This produces an upper bound on the negative log-likelihood.</p> \[\le \mathbb{E}_q\left[ -\log \frac{p_\theta(x_{0:T})}{q(x_{1:T}\mid x_0)} \right]\] <p><strong>Step 4</strong>: Next, factorize the model and variational distributions</p> \[p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^{T} p_\theta(x_{t-1}\mid x_t), \quad q(x_{1:T}\mid x_0) = \prod_{t=1}^{T} q(x_t\mid x_{t-1})\] <p><strong>Step 5</strong>: Substituting these factorizations:</p> \[= \mathbb{E}_q\left[ -\log p(x_T)- \sum_{t=1}^{T} \log \frac{p_\theta(x_{t-1}\mid x_t)} {q(x_t\mid x_{t-1})} \right] := \mathcal{L}\] <p>The first term \(\mathbb{E}_q[-\log p(x_T)]\) is fixed and not optimizable, so we optimize the second term:</p> \[\mathbb{E}_{q(x_{t-1}, x_t)} \left[ \log \frac{q(x_t \mid x_{t-1})} {p_\theta(x_{t-1} \mid x_t)} \right] = \mathbb{E}_{q(x_0, x_t)} \Big[ \mathrm{KL}\big( q(x_{t-1} \mid x_t, x_0) \;\|\; p_\theta(x_{t-1} \mid x_t) \big) \Big]+\text{const}\] <p>this is equavalent to a noise-prediction objective, where a neural network \(\epsilon_\theta\) (typically a UNet) minimizes the error between the added noise \(\epsilon\) and the predicted noise:</p> \[L = \mathbb{E} [ \lambda(t)||\epsilon - \epsilon_\theta(x_t, t)||^2 ]\] </blockquote> <h3 id="212-score-based-generative-models-sgms-and-sdes">2.1.2 Score-based Generative Models (SGMs) and SDEs</h3> <p>Alternative formulations focus on the gradient of the log-density of the data instead of noise being added, known as the score function \(\nabla_x \log p(x)\).</p> <ul> <li><strong>SGMs:</strong> In score-based generative modeling, we train a <strong>Noise-Conditioned Score Network (NCSN)</strong> \(s_\theta(x,t)\) to approximate the score (gradient of the log-density) of data corrupted by Gaussian noise. The classical score-matching objective is</li> </ul> \[\frac{1}{2}\,\mathbb{E}_{p_{\text{data}}} \big\| s_\theta(x) - \nabla_x \log p_{\text{data}}(x) \big\|^2 .\] <p>Let \(x_0 \sim q(x_0)\) denote clean data, and define a family of noisy distributions via</p> \[q(x_t \mid x_0) = \mathcal{N}(x_t; x_0, \sigma_t^2 I), \qquad q(x_t) = \int q(x_t \mid x_0) q(x_0)\,dx_0 .\] <p>Conditioning the score network on the noise level \(t\), the score-matching objective becomes (up to a constant factor):</p> \[\mathbb{E}_{t \sim \mathcal{U}[1,T],\, x_0 \sim q(x_0),\, x_t \sim q(x_t \mid x_0)} \left[\lambda(t)\, \big\| s_\theta(x_t, t) - \nabla_x \log q(x_t \mid x_0) \big\|^2 \right].\] <p>The relation between DDPM and SGM is then</p> \[\boxed{ \epsilon_\theta(x,t) = -\,\sigma_t\, s_\theta(x,t) }\] <ul> <li><strong>SDEs:</strong> While DDPMs and SGMs are originally defined using a <strong>finite, discretized noising process</strong>, the diffusion process can be equivalently formulated in <strong>continuous time</strong> as a stochastic differential equation (SDE). This continuous view unifies diffusion models and enables flexible sampling algorithms.</li> </ul> <h4 id="general-forward-diffusion-sde">General Forward Diffusion SDE</h4> <p>The forward noising process is described by the <strong>Score SDE</strong>:</p> \[dx = f(x,t)\,dt + g(t)\,dw,\] <p>where:</p> <ul> <li>\(f(x,t)\) is the <strong>drift (diffusion) term</strong>,</li> <li>\(g(t)\) controls the <strong>noise magnitude</strong>,</li> <li>\(w\) is a standard Wiener process.</li> </ul> <p>This SDE defines a family of marginal distributions \(q_t(x)\) over time.</p> <h4 id="ddpm-as-a-special-case">DDPM as a Special Case</h4> <p>The continuous-time limit of DDPM corresponds to the SDE:</p> \[dx = -\frac{1}{2}\beta(t)\,x\,dt + \sqrt{\beta(t)}\,dw,\] <p>where \(\beta(t)\) is a continuous noise schedule. This formulation mirrors the discrete DDPM noising process in the limit of infinitely many steps.</p> <h4 id="sgm-as-a-special-case">SGM as a Special Case</h4> <p>Score-based Generative Models (SGMs) correspond to the SDE:</p> \[dx = \sqrt{\frac{d[\sigma_t^2]}{dt}}\,dw,\] <p>where \(\sigma(t)\) is the continuous noise scale. Here, the forward process is a pure diffusion without drift.</p> <h4 id="reverse-time-sde-sampling-process">Reverse-Time SDE (Sampling Process)</h4> <p>For any forward SDE of the form above, the <strong>reverse-time SDE</strong> is given by:</p> \[dx = \big[f(x,t) - g(t)^2 \nabla_x \log q_t(x)\big]\,dt + g(t)\,d\bar{w},\] <p>where:</p> <ul> <li>\(\nabla_x \log q_t(x)\) is the <strong>score function</strong>,</li> <li>\(\bar{w}\) is a backward-time Wiener process.</li> </ul> <p>Learning the score function enables generation of samples by simulating this reverse SDE.</p> <h4 id="probability-flow-ode">Probability Flow ODE</h4> <p>In addition to the reverse SDE, there exists an equivalent <strong>deterministic ODE</strong> with identical marginal distributions:</p> \[dx = \left[f(x,t) - \frac{1}{2}g(t)^2 \nabla_x \log q_t(x)\right]\,dt.\] <p>This <strong>probability flow ODE</strong> allows sampling without stochasticity, using standard ODE solvers.</p> <h4 id="sampling-methods">Sampling Methods</h4> <p>Given the learned score function, samples can be generated using:</p> <ul> <li>Reverse-time SDE solvers,</li> <li>Probability flow ODE solvers,</li> <li>Annealed Langevin Dynamics,</li> <li>Predictor–Corrector (PC) samplers combining SDE solvers with MCMC methods (e.g., Langevin MCMC or HMC).</li> </ul> <h2 id="22-conditional-generation-mechanisms">2.2 Conditional Generation Mechanisms</h2> <table> <tbody> <tr> <td>To enable controllable generation (e.g., text-to-image synthesis), diffusion models must incorporate a conditional vector $c$. The reverse process is modified to $p_\theta(x_{t-1}</td> <td>x_t, c)$. Two primary guidance methods dominate the literature:</td> </tr> </tbody> </table> <h3 id="221-classifier-guidance">2.2.1 Classifier Guidance</h3> <p>This approach leverages an auxiliary classifier $p_\phi(c | x_t)$ trained on noisy images. The denoising process can be expressed: \(p_{\theta,\phi}(x_t \mid x_{t+1}, c)=Z \, p_\theta(x_t \mid x_{t+1}) \, p_\phi(c \mid x_t)\) where \(Z\) is a normalization constant. Taking the gradient of the log of the conditional density (ignoring \(Z\)):</p> \[\nabla_{x_t} \log \big( p_\theta(x_t \mid x_{t+1}) \, p_\phi(c \mid x_t) \big)=\nabla_{x_t} \log p_\theta(x_t \mid x_{t+1}) + \nabla_{x_t} \log p_\phi(c \mid x_t).\] <p>Using the relationship between score and noise prediction,</p> \[\epsilon_\theta(x_t, t) = -\sigma_t s_\theta(x_t, t),\] <p>the gradient becomes:</p> <p>\(=-\frac{1}{\sigma_t}\,\epsilon_\theta(x_t, t)+\nabla_{x_t} \log p_\phi(c \mid x_t).\)</p> <blockquote> <p><strong>Key Intuition:</strong> beside the original unconditional score function, the gradient of the classifier is added to guide the denoise process.</p> </blockquote> <h3 id="222-classifier-free-guidance">2.2.2 Classifier-free Guidance</h3> <p>To avoid the computational cost and complexity of training a separate noise-robust classifier, Ho and Salimans (2022) proposed classifier-free guidance. Here, a single diffusion model is trained to handle both conditional and unconditional inputs (where $c = \emptyset$). During sampling, the noise prediction is a linear combination of both outputs, weighted by a scale $w$:</p> \[\tilde{\epsilon}_\theta(x_t, c) = (1 + w)\epsilon_\theta(x_t, c) - w\epsilon_\theta(x_t)\] <p>This method implicitly maximizes the probability of the condition without an external classifier and has become the standard for state-of-the-art models.</p> <blockquote> <p><strong>Key Intuition:</strong> trains one diffusion model to operate in two modes: with condition and without condition. During sampling, the model compares these two predictions. The difference tells you how the condition should push the sample, and scaling that difference lets you control how strongly the generation follows the condition. In effect, the model learns its own “internal classifier gradient” and uses it to guide sampling—without ever training an explicit classifier.</p> </blockquote> <h2 id="23-state-of-the-art-architectures">2.3 State-of-the-Art Architectures</h2> <p>Current text-to-image systems are generally categorized by whether the diffusion process occurs in pixel space or latent space.</p> <h3 id="231-pixel-based-models">2.3.1 Pixel-based Models</h3> <p>These models operate directly on high-dimensional image data.</p> <ul> <li><strong>GLIDE:</strong> Uses classifier-free guidance to generate photorealistic images and demonstrates capabilities in text-guided inpainting [15].</li> <li><strong>Imagen:</strong> A key finding from the development of Imagen is the scaling law regarding text encoders. The authors discovered that increasing the size of the language model (e.g., using T5-XXL) yields greater improvements in image fidelity and image-text alignment than increasing the size of the visual diffusion model itself [16].</li> </ul> <h3 id="232-latent-based-models">2.3.2 Latent-based Models</h3> <p>To address the high computational costs of pixel-space diffusion, <strong>Latent Diffusion Models (LDMs)</strong> utilize an autoencoder to project data into a lower-dimensional latent space [18].</p> <ul> <li><strong>Stable Diffusion:</strong> Applies the diffusion process within this compressed latent space, utilizing cross-attention mechanisms to incorporate text conditioning. This architecture significantly improves inference efficiency [19].</li> <li><strong>DALL-E 2 (unCLIP):</strong> Utilizes the CLIP embedding space. It generates an image embedding from text and then decodes this embedding into an image, leveraging the joint multimodal space learned by CLIP [20].</li> </ul> <h2 id="24-failure-modes-and-limitations">2.4 Failure Modes and Limitations</h2> <p>Despite the fidelity of these models, systematic evaluations reveal persistent limitations in their reasoning capabilities:</p> <ol> <li><strong>Attribute Binding:</strong> Models frequently fail to correctly bind attributes to objects. For example, in a prompt specifying a “red cube” and a “blue cube,” DALL-E 2 may swap the colors or textures [22], [23].</li> <li><strong>Text Rendering:</strong> While semantic understanding is high, the ability to render coherent alphanumeric text remains poor, likely due to tokenization schemes (BPE encoding) that obscure spelling information from the model [23], [24].</li> <li><strong>Physical Consistency:</strong> Generated images often exhibit violations of physical laws, such as incorrect shadow placement or reflections that do not align with the object’s geometry [25].</li> <li><strong>Bias toward Canonical Forms:</strong> When prompted with unusual scenarios (e.g., “a car with triangular wheels”), models often revert to the mean of the training distribution (circular wheels), indicating a lack of true compositional generalization [26].</li> </ol> <p>These failure modes suggest that while diffusion models excel at texture synthesis and semantic association, they struggle with precise factorization and compositional reasoning—a core focus of the subsequent chapters of this thesis [27].</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="Learning"/><summary type="html"><![CDATA[A learning notebook for diffusion models]]></summary></entry></feed>