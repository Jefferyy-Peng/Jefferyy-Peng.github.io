<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jefferyy-peng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jefferyy-peng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-10T00:01:31+00:00</updated><id>https://jefferyy-peng.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Generalization Through Variance in Diffusion Models</title><link href="https://jefferyy-peng.github.io/blog/2025/diffusion-generalization-variance/" rel="alternate" type="text/html" title="Generalization Through Variance in Diffusion Models"/><published>2025-11-21T16:40:16+00:00</published><updated>2025-11-21T16:40:16+00:00</updated><id>https://jefferyy-peng.github.io/blog/2025/diffusion-generalization-variance</id><content type="html" xml:base="https://jefferyy-peng.github.io/blog/2025/diffusion-generalization-variance/"><![CDATA[<p><strong>TL;DR:</strong> Optimizing locally instead of optimizing a global score function introduces structured noise; this noise induces variance in the learned score, which in turn induces generalization.</p> <h2 id="0-high-level-roadmap-what-the-paper-tries-to-do">0. High-level roadmap (what the paper tries to do)</h2> <p>The paper’s main goal is to analytically characterize the <strong>typical learned sampling distribution</strong> of diffusion models trained with denoising score matching (DSM), and to explain <strong>why they generalize</strong> (place probability mass between training examples) instead of perfectly memorizing.</p> <p>Key steps:</p> <ol> <li>Define forward diffusion and reverse PF-ODE sampling.</li> <li>Compare the “true-score” objective \(J_0\) vs DSM objective \(J_1\).</li> <li>Show the proxy score is <strong>unbiased</strong> but has a structured <strong>covariance</strong>.</li> <li>Treat the trained score estimator \(\hat s_\theta\) as random (because it depends on finite training samples).</li> <li>Write PF-ODE sampling as a <strong>path integral</strong>, so that averaging over training randomness becomes tractable.</li> <li>Perform the ensemble average via cumulants, producing mean term \(M_1\) and covariance term \(M_2\) (the <strong>V-kernel</strong>).</li> <li>Under a Gaussian approximation, show the averaged dynamics are equivalent to an <strong>effective SDE</strong>: generalization happens iff the V-kernel is nonzero.</li> <li>Provide a toy “naive estimator” scheme showing how nonzero V-kernel arises even when using the proxy score directly.</li> </ol> <hr/> <h2 id="1-preliminaries-distributions-conditionals-and-marginals">1. Preliminaries: distributions, conditionals, and marginals</h2> <h3 id="11-data-distribution">1.1 Data distribution</h3> <p>Let \(x_0 \in \mathbb R^D\) denote clean data. The data distribution is \(p_{\text{data}}(x_0)\).</p> <p>A common idealization used in the paper is an empirical distribution over \(M\) examples \(\{\mu_m\}_{m=1}^M\): \(p_{\text{data}}(x_0) = \frac{1}{M}\sum_{m=1}^M \delta(x_0 - \mu_m).\)</p> <p>Here \(\delta(\cdot)\) is the Dirac delta distribution.</p> <hr/> <h2 id="2-forward-diffusion">2. Forward diffusion</h2> <h3 id="21-forward-sde">2.1 Forward SDE</h3> <p>The forward process is an SDE: \(\dot x_t = -\beta_t x_t + G_t \eta_t,\quad t:0\to T.\) Definitions:</p> <ul> <li>\(x_t \in \mathbb R^D\): random variable at time \(t\).</li> <li>\(\dot x_t\): time derivative (informally; in SDE form it corresponds to Ito dynamics).</li> <li>\(\beta_t \ge 0\): scalar drift schedule.</li> <li>\(G_t \in \mathbb R^{D\times K}\): noise injection matrix.</li> <li>\(\eta_t\): standard Gaussian white noise process.</li> </ul> <p>Define diffusion tensor: \(D_t := \frac{G_t G_t^\top}{2}\in \mathbb R^{D\times D}.\)</p> <hr/> <h3 id="22-solve-the-forward-sde-derive-px_tmid-x_0tmathcal-nalpha_t-x_0-s_t">2.2 Solve the forward SDE: derive \(p(x_t\mid x_0,t)=\mathcal N(\alpha_t x_0, S_t)\)</h3> <p>We derive the conditional distribution of \(x_t\) given \(x_0\).</p> <p>Write the SDE in Ito differential form: \(dx_t = -\beta_t x_t dt + G_t dW_t,\) where \(W_t\) is a \(K\)-dim Brownian motion (so \(dW_t\) are Gaussian increments).</p> <h4 id="step-1-integrating-factor">Step 1: integrating factor</h4> <p>Define \(\alpha_t := \exp\left(-\int_0^t \beta_{t'} dt'\right).\) Note: \(\frac{d}{dt}\alpha_t = -\beta_t \alpha_t,\quad \alpha_0=1.\)</p> <p>Consider the scaled process: \(y_t := \alpha_t^{-1} x_t.\)</p> <p>Use Ito (here drift-only scaling works as usual since factor is deterministic): \(dy_t = d(\alpha_t^{-1} x_t) = \alpha_t^{-1} dx_t + x_t d(\alpha_t^{-1}).\) Compute: \(d(\alpha_t^{-1}) = -\alpha_t^{-2} d\alpha_t = -\alpha_t^{-2}(-\beta_t \alpha_t dt) = \beta_t \alpha_t^{-1} dt.\) So: \(dy_t = \alpha_t^{-1}(-\beta_t x_t dt + G_t dW_t) + x_t(\beta_t \alpha_t^{-1} dt) = \alpha_t^{-1} G_t dW_t.\)</p> <h4 id="step-2-integrate">Step 2: integrate</h4> <p>Integrate from 0 to t: \(y_t = y_0 + \int_0^t \alpha_{t'}^{-1} G_{t'} dW_{t'}.\) But \(y_0 = \alpha_0^{-1} x_0 = x_0\). Thus: \(y_t = x_0 + \int_0^t \alpha_{t'}^{-1} G_{t'} dW_{t'}.\)</p> <p>Multiply by \(\alpha_t\): \(x_t = \alpha_t x_0 + \alpha_t\int_0^t \alpha_{t'}^{-1} G_{t'} dW_{t'}.\)</p> <h4 id="step-3-identify-gaussian-distribution">Step 3: identify Gaussian distribution</h4> <p>The stochastic integral is Gaussian with mean 0. Thus conditional on \(x_0\):</p> <ul> <li>mean: \(\mathbb E[x_t\mid x_0] = \alpha_t x_0.\)</li> <li>covariance: Let \(\varepsilon_t := \alpha_t\int_0^t \alpha_{t'}^{-1} G_{t'} dW_{t'}.\) Then: \(\mathrm{Cov}(\varepsilon_t\mid x_0) = \alpha_t^2 \int_0^t \alpha_{t'}^{-2} G_{t'} \mathrm{Cov}(dW_{t'}) G_{t'}^\top.\) Since \(\mathrm{Cov}(dW_{t'}) = I dt'\): \(\mathrm{Cov}(\varepsilon_t\mid x_0) = \alpha_t^2 \int_0^t \alpha_{t'}^{-2} G_{t'} G_{t'}^\top dt' = \alpha_t^2 \int_0^t \alpha_{t'}^{-2} (2D_{t'}) dt'.\)</li> </ul> <p>Many texts rewrite this as an equivalent expression in terms of the forward-time convention used in the paper: \(S_t := \int_0^t 2D_{t'} \alpha_{t'}^2 dt'.\) (This matches the paper’s definition; it can be obtained by a change of variables depending on whether one defines \(\alpha_t\) relative to 0 or relative to t. The paper uses the above closed form.)</p> <p>Thus: \(p(x\mid x_0,t) = \mathcal N(x; \alpha_t x_0, S_t).\)</p> <hr/> <h3 id="23-derive-the-marginal-at-time-t">2.3 Derive the marginal at time \(t\)</h3> <p>Define the marginal: \(p(x\mid t) := \int p(x\mid x_0,t)\, p_{\text{data}}(x_0)\, dx_0.\)</p> <p><strong>Derivation:</strong> law of total probability / marginalization: \(p(x\mid t) = \int p(x,x_0\mid t)\, dx_0 = \int p(x\mid x_0,t)p_{\text{data}}(x_0)\, dx_0.\)</p> <p>If \(p_{\text{data}}\) is discrete mixture of deltas: \(p(x\mid t) = \frac{1}{M}\sum_{m=1}^M \mathcal N(x; \alpha_t \mu_m, S_t).\) So the forward marginal becomes a Gaussian mixture with components centered at scaled training examples.</p> <h3 id="24-reverse-process-as-an-ode">2.4 Reverse process as an ODE</h3> \[\dot x_t = -\beta_t x_t - D_t\, s(x_t, t),\quad t:T\to \epsilon.\] <p>This is a Probability Flow ODE (PF-ODE), which is equivalent to the reverse SDE for \(p(x_t)\) for all \(t\).</p> <hr/> <h2 id="3-scores-true-score-and-proxy-score-with-full-derivations">3. Scores: true score and proxy score (with full derivations)</h2> <h3 id="31-true-score">3.1 True score</h3> <p>Define the true score: \(s(x,t) := \nabla_x \log p(x\mid t).\)</p> <p>This is the vector field needed by reverse-time sampling methods.</p> <hr/> <h3 id="32-proxy-score-dsm-target-and-its-closed-form">3.2 Proxy score (DSM target) and its closed form</h3> <p>The true score is unknown. To approximate the true score, the proxy score is defined as the score on each data sample, which have a gaussian form: \(\tilde s(x,t;x_0) := \nabla_x \log p(x\mid x_0,t).\) Since: \(p(x\mid x_0,t)=\mathcal N(x; \mu, \Sigma),\quad \mu=\alpha_t x_0,\quad \Sigma=S_t,\) we compute: \(\log \mathcal N(x;\mu,\Sigma) = -\frac{D}{2}\log(2\pi) - \frac{1}{2}\log\det\Sigma - \frac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu).\) Differentiate w.r.t. \(x\): \(\nabla_x \log \mathcal N(x;\mu,\Sigma) = -\frac{1}{2}\nabla_x\left((x-\mu)^\top \Sigma^{-1}(x-\mu)\right).\) Using: \(\nabla_x\left((x-\mu)^\top A (x-\mu)\right)= 2A(x-\mu)\quad \text{for symmetric }A,\) we get: \(\nabla_x \log \mathcal N(x;\mu,\Sigma)= -\Sigma^{-1}(x-\mu) = \Sigma^{-1}(\mu-x).\) Substitute \(\mu\) and \(\Sigma\): \(\tilde s(x,t;x_0) = S_t^{-1}(\alpha_t x_0 - x).\)</p> <hr/> <h2 id="4-compare-objectives-j_0-vs-j_1-and-why-tilde-s-matters">4. Compare objectives \(J_0\) vs \(J_1\) and why \(\tilde s\) matters</h2> <h3 id="41-define-the-objectives">4.1 Define the objectives</h3> <p>Idealized objective: \(J_0(\theta) := \mathbb E_{t,x}\left[\frac{\lambda_t}{2}\|\hat s_\theta(x,t) - s(x,t)\|^2\right].\)</p> <p>DSM objective: \(J_1(\theta) := \mathbb E_{t,x_0,x}\left[\frac{\lambda_t}{2}\|\hat s_\theta(x,t) - \tilde s(x,t;x_0)\|^2\right].\)</p> <p>Here the sampling is:</p> <ul> <li> \[t\sim p(t)\] </li> <li> \[x_0\sim p_{\text{data}}(x_0)\] </li> <li> \[x\sim p(x\mid x_0,t)\] </li> </ul> <hr/> <h3 id="42-unbiasedness-derive-mathbb-e_x_0mid-xttilde-ss">4.2 Unbiasedness: derive \(\mathbb E_{x_0\mid x,t}[\tilde s]=s\)</h3> <p>They prove that the expectation of the proxy score is equal to the true score: \(\mathbb E_{x_0\mid x,t}[\tilde s(x,t;x_0)] = s(x,t).\)</p> <p>Start from definition: \(p(x\mid t)=\int p(x\mid x_0,t)p_{\text{data}}(x_0)\, dx_0.\) Differentiate w.r.t. \(x\): \(\nabla_x p(x\mid t) = \int \nabla_x p(x\mid x_0,t)p_{\text{data}}(x_0)\, dx_0.\) Rewrite: \(\nabla_x p(x\mid x_0,t) = p(x\mid x_0,t)\nabla_x \log p(x\mid x_0,t) = p(x\mid x_0,t)\tilde s(x,t;x_0).\) Thus: \(\nabla_x p(x\mid t)=\int p(x\mid x_0,t)\tilde s(x,t;x_0)p_{\text{data}}(x_0)\, dx_0.\) But: \(p(x_0\mid x,t) = \frac{p(x\mid x_0,t)p_{\text{data}}(x_0)}{p(x\mid t)}.\) So divide by \(p(x\mid t)\): \(\frac{\nabla_x p(x\mid t)}{p(x\mid t)} = \int \tilde s(x,t;x_0) p(x_0\mid x,t)\, dx_0.\) Left-hand side equals: \(\nabla_x \log p(x\mid t) = s(x,t).\) Therefore: \(s(x,t)=\mathbb E_{x_0\mid x,t}[\tilde s(x,t;x_0)].\)</p> <hr/> <h3 id="43-covariance-of-proxy-score">4.3 Covariance of proxy score</h3> <p>Define: \(C_{ij}(x,t) := \mathrm{Cov}_{x_0\mid x,t}[\tilde s_i, \tilde s_j] = \mathbb E[\tilde s_i\tilde s_j] - s_i s_j,\) where expectations are under \(p(x_0\mid x,t)\).</p> <p>We want the identity: \(C(x,t)=S_t^{-1} + \nabla_x^2 \log p(x\mid t).\)</p> <h4 id="step-1-compute-hessian-of-log-marginal">Step 1: compute Hessian of log marginal</h4> <p>Start with: \(s(x,t)=\nabla_x \log p(x\mid t)=\frac{\nabla_x p(x\mid t)}{p(x\mid t)}.\) Differentiate again: \(\nabla_x^2 \log p = \nabla_x\left(\frac{\nabla_x p}{p}\right) = \frac{\nabla_x^2 p}{p} - \frac{(\nabla_x p)(\nabla_x p)^\top}{p^2}.\) So: \(\nabla_x^2 \log p = \frac{\nabla_x^2 p}{p} - s s^\top.\)</p> <h4 id="step-2-express-nabla_x2-pxmid-t-in-terms-of-conditional-objects">Step 2: express \(\nabla_x^2 p(x\mid t)\) in terms of conditional objects</h4> <p>We have: \(p(x\mid t)=\int p(x\mid x_0,t)p_{\text{data}}(x_0)\, dx_0.\) Differentiate twice: \(\nabla_x^2 p(x\mid t)=\int \nabla_x^2 p(x\mid x_0,t)p_{\text{data}}(x_0)\, dx_0.\)</p> <p>Now compute \(\nabla_x^2 p(x\mid x_0,t)\): \(\nabla_x^2 p = \nabla_x(p\tilde s)= (\nabla_x p)\tilde s^\top + p\nabla_x\tilde s = p\tilde s\tilde s^\top + p\nabla_x\tilde s.\) Thus: \(\nabla_x^2 p(x\mid x_0,t) = p(x\mid x_0,t)\left(\tilde s\tilde s^\top + \nabla_x\tilde s\right).\)</p> <p>Integrate: \(\nabla_x^2 p(x\mid t)=\int p(x\mid x_0,t)\left(\tilde s\tilde s^\top + \nabla_x\tilde s\right)p_{\text{data}}(x_0)\, dx_0.\) Divide by \(p(x\mid t)\) to convert to posterior expectation: \(\frac{\nabla_x^2 p(x\mid t)}{p(x\mid t)} = \mathbb E_{x_0\mid x,t}\left[\tilde s\tilde s^\top + \nabla_x\tilde s\right].\)</p> <h4 id="step-3-substitute-into-hessian-identity">Step 3: substitute into Hessian identity</h4> <p>Recall: \(\nabla_x^2 \log p = \frac{\nabla_x^2 p}{p} - s s^\top.\) So: \(\nabla_x^2 \log p = \mathbb E[\tilde s\tilde s^\top + \nabla_x\tilde s] - s s^\top = \mathrm{Cov}(\tilde s) + \mathbb E[\nabla_x\tilde s].\) Therefore: \(\mathrm{Cov}(\tilde s) = \nabla_x^2 \log p - \mathbb E[\nabla_x\tilde s].\)</p> <h4 id="step-4-compute-nabla_xtilde-s-for-gaussian-conditional">Step 4: compute \(\nabla_x\tilde s\) for Gaussian conditional</h4> <p>We have: \(\tilde s(x,t;x_0)=S_t^{-1}(\alpha_t x_0 - x).\) Differentiate w.r.t. \(x\): \(\nabla_x\tilde s = \nabla_x(-S_t^{-1} x) = -S_t^{-1}.\) This is constant (independent of \(x_0\)). So: \(\mathbb E_{x_0\mid x,t}[\nabla_x\tilde s] = -S_t^{-1}.\)</p> <p>Substitute: \(\mathrm{Cov}(\tilde s) = \nabla_x^2 \log p - (-S_t^{-1}) = S_t^{-1} + \nabla_x^2 \log p.\)</p> <p>So: \(C(x,t)=S_t^{-1} + \nabla_x^2 \log p(x\mid t).\)</p> <p><strong>Interpretation (from our discussion):</strong></p> <ul> <li>As \(t\to 0\), \(S_t\to 0\) so \(S_t^{-1}\) blows up: proxy-score covariance large at small times.</li> <li>Large curvature \(\nabla_x^2 \log p\) happens near modes and boundaries (e.g., for discrete mixtures).</li> </ul> <hr/> <h3 id="44-why-proxy-score-covariance-matters-for-training-and-generalization">4.4 Why proxy-score covariance matters for training and generalization</h3> <p>Even though \(\tilde s\) is unbiased for \(s\), training on \(J_1\) uses noisy targets. Empirically, finite neural nets trained on \(J_1\) produce a sampling distribution different from the true-score PF-ODE.</p> <p>The paper’s thesis: <strong>structured noise in the target induces structured variance in the learned estimator</strong>, which translates into stochasticity in typical sampling, yielding generalization.</p> <hr/> <h2 id="5-pf-ode-sampling-and-the-distribution-of-outputs">5. PF-ODE sampling and the “distribution of outputs”</h2> <h3 id="51-pf-ode-dynamics">5.1 PF-ODE dynamics</h3> <p>Sampling uses the probability flow ODE: \(\dot x_t = -\beta_t x_t - D_t s(x_t,t),\quad t:T\to \epsilon.\) In practice, you plug in the learned score estimator \(\hat s_\theta\): \(\dot x_t = -\beta_t x_t - D_t \hat s_\theta(x_t,t).\)</p> <h3 id="52-deterministic-mapping-given-theta">5.2 Deterministic mapping given \(\theta\)</h3> <p>Fix:</p> <ul> <li>the network parameters \(\theta\)</li> <li>the initial noise seed \(x_T\)</li> </ul> <p>Then the ODE solution is deterministic: \(x_0 = F(x_T,\theta).\)</p> <h3 id="53-why-the-output-distribution-uses-a-delta">5.3 Why the output distribution uses a delta</h3> <p>The typical distribution of the generated sample \(\bar q(x_0\mid x_T)\) is defined:</p> \[\bar q(x_0\mid x_T) = \mathbb E_\theta[\delta(x_0 - F(x_T,\theta))]?\] <p>Because this is the standard identity: for any random variable \(Y\), its density can be written as: \(p_Y(y) = \mathbb E[\delta(y-Y)].\)</p> <h4 id="definition-of-dirac-delta">Definition of Dirac delta</h4> <p>The delta is defined by its action under integration: \(\int \delta(x-a) f(x) dx = f(a).\) Thus, if \(Y\) is deterministic with value \(a\), its distribution is \(\delta(x-a)\).</p> <p>Here \(Y=F(x_T,\theta)\) is random only through \(\theta\); averaging those deltas gives the typical distribution.</p> <hr/> <h2 id="6-key-technical-tool-path-integral-representation-of-pf-ode-outputs-eq-6">6. Key technical tool: path integral representation of PF-ODE outputs (Eq. 6)</h2> <h3 id="61-why-we-need-it">6.1 Why we need it</h3> <p>Directly averaging over \(F(x_T,\theta)\) is hard because:</p> <ul> <li>PF-ODE typically has no closed-form solution.</li> <li>\(F\) depends on \(\hat s_\theta\) in a complicated, nonlinear, trajectory-dependent way.</li> </ul> <p>So the paper rewrites the deterministic constraint “this path satisfies the ODE” into an integral form where \(\hat s_\theta\) appears linearly in an exponent.</p> <hr/> <h3 id="62-discrete-time-derivation-of-the-path-integral-idea-most-intuitive">6.2 Discrete-time derivation of the path integral idea (most intuitive)</h3> <p>Discretize time: \(t_k = T - k\Delta t,\quad k=0,1,\dots,N,\quad t_N=\epsilon.\)</p> <p>Euler update for PF-ODE using estimator: \(x_{k+1} = x_k + \Delta t\left(\beta_{t_k} x_k + D_{t_k}\hat s_\theta(x_k,t_k)\right).\) Rearrange: \(x_{k+1} - x_k - \Delta t(\beta_{t_k}x_k + D_{t_k}\hat s_\theta(x_k,t_k)) = 0.\)</p> <p>Enforce each step by a delta: \(\delta\left(x_{k+1}-x_k-\Delta t(\beta_{t_k}x_k + D_{t_k}\hat s_\theta(x_k,t_k))\right).\)</p> <p>Now enforce delta via Fourier representation (finite-dimensional): \(\delta(y) = \int \frac{dp}{(2\pi)^D}\exp(ip\cdot y).\)</p> <p>Introduce an auxiliary \(p_k\) per time step: \(\delta(\cdots) = \int dp_k \exp\left(ip_k\cdot\left[x_{k+1}-x_k-\Delta t(\beta_{t_k}x_k + D_{t_k}\hat s_\theta(x_k,t_k))\right]\right).\)</p> <p>Multiply over all steps and integrate over intermediate states \(\{x_k\}\) and auxiliaries \(\{p_k\}\). In the continuous-time limit, this becomes the functional integral (Eq. 6): \(q(x_0\mid x_T;\theta) = \int \mathcal D[p_t]\mathcal D[x_t]\, \exp\left( \int_T^\epsilon i p_t\cdot[\dot x_t+\beta_t x_t + D_t \hat s_\theta(x_t,t)]dt \right).\)</p> <p>Because integrating over \(p_t\) creates a delta-functional: \(\int \mathcal D[p_t]\exp\left(i\int p_t\cdot R_t[x]dt\right) \propto \delta[R_t[x]].\) This delta-functional equals “1” on paths where \(R_t[x]=0\) (ODE satisfied), and “0” otherwise. So ODE-consistent paths contribute by <strong>surviving</strong>, not by making the exponent nonzero.</p> <hr/> <h2 id="7-ensemble-averaging-cumulant-expansion-to-get-eq-7">7. Ensemble averaging: cumulant expansion to get Eq. 7</h2> <h3 id="71-what-is-random">7.1 What is random?</h3> <p>The score estimator \(\hat s_\theta(x,t)\) is random because \(\theta\) depends on random finite training data (and optimization randomness). So \(q(x_0\mid x_T;\theta)\) is random, and we want its ensemble average: \([q(x_0\mid x_T)] := \mathbb E_\theta[q(x_0\mid x_T;\theta)].\)</p> <h3 id="72-why-the-average-becomes-tractable">7.2 Why the average becomes tractable</h3> <p>Inside Eq. 6, the dependence on \(\hat s_\theta\) is linear in the exponent: \(\exp\left(\int i p_t\cdot D_t \hat s_\theta(x_t,t)dt\right).\) So we need to average an exponential of a linear functional: \(\mathbb E_\theta\left[\exp\left(\int J_t\cdot \hat s_\theta(x_t,t) dt\right)\right],\) where \(J_t := i D_t^\top p_t\).</p> <p>This is a <strong>characteristic functional</strong>. Its log admits a cumulant expansion: \(\log \mathbb E[e^{Z}] = \kappa_1(Z) + \frac{1}{2}\kappa_2(Z) + \frac{1}{6}\kappa_3(Z)+\cdots,\) where \(\kappa_n\) are cumulants.</p> <p>With \(Z\) linear in \(\hat s_\theta\):</p> <ul> <li>\(\kappa_1\) depends on the mean of \(\hat s_\theta\)</li> <li>\(\kappa_2\) depends on the covariance of \(\hat s_\theta\)</li> <li>higher cumulants correspond to non-Gaussianity</li> </ul> <p>Thus the paper obtains: \([q(x_0\mid x_T)] = \int \mathcal D[p_t]\mathcal D[x_t]\, \exp\left(M_1 - \frac{1}{2}M_2 + \cdots\right).\)</p> <h3 id="73-define-s_textavg-and-v-kernel-precisely">7.3 Define \(s_{\text{avg}}\) and V-kernel precisely</h3> <p>Mean score: \(s_{\text{avg}}(x,t) := [\hat s_\theta(x,t)] = \mathbb E_\theta[\hat s_\theta(x,t)].\)</p> <p>Covariance kernel: \(\mathrm{Cov}_\theta[\hat s(x,t),\hat s(x',t')] := \mathbb E_\theta[(\hat s-s_{\text{avg}})(\hat s'-s_{\text{avg}}')^\top].\)</p> <p>V-kernel (as defined by paper): \(V(x,t;x',t') := D_t \mathrm{Cov}_\theta[\hat s(x,t),\hat s(x',t')] D_{t'}.\)</p> <h3 id="74-write-m_1-and-m_2">7.4 Write \(M_1\) and \(M_2\)</h3> <p>\(M_1 := \int_T^\epsilon i p_t\cdot[\dot x_t + \beta_t x_t + D_t s_{\text{avg}}(x_t,t)]dt,\) \(M_2 := \int_T^\epsilon\int_T^\epsilon p_t^\top V(x_t,t;x_{t'},t') p_{t'} dt dt'.\)</p> <hr/> <h2 id="8-gaussian-approximation-and-the-effective-sde-eq-8--proposition-31">8. Gaussian approximation and the Effective SDE (Eq. 8 / Proposition 3.1)</h2> <h3 id="81-dropping-higher-cumulants">8.1 Dropping higher cumulants</h3> <p>Assume the distribution of the estimator across training randomness is approximately Gaussian. Then higher cumulants vanish, so only \(M_1\) and \(M_2\) remain.</p> <h3 id="82-from-quadratic-form-to-noise-the-key-idea">8.2 From quadratic form to noise: the key idea</h3> <p>With a Gaussian field, averaging produces a quadratic term \(M_2\) in the exponent. Quadratic terms in an action correspond to Gaussian noise in an equivalent SDE description.</p> <p>The result: sampling from \([q(x_0\mid x_T)]\) is equivalent to integrating an SDE: \(\dot x_t = -\beta_t x_t - D_t s_{\text{avg}}(x_t,t) + \xi(x_t,t),\) with: \(\mathbb E[\xi(x_t,t)]=0,\) \(\mathbb E[\xi(x_t,t)\xi(x_{t'},t')^\top] = V(x_t,t;x_{t'},t').\)</p> <h3 id="83-key-conceptual-consequence-the-papers-main-interpretive-statement">8.3 Key conceptual consequence (the paper’s main interpretive statement)</h3> <ul> <li>If \(\hat s\) is unbiased so \(s_{\text{avg}}=s\),</li> <li>then deterministic drift matches the true PF-ODE,</li> <li>and the only difference is the noise term controlled by V-kernel.</li> </ul> <p>Thus:</p> <ul> <li>\(V=0\) implies the effective dynamics reduce to deterministic PF-ODE, which (for discrete training data) reproduces training examples (memorization).</li> <li>\(V\ne 0\) implies added stochasticity, causing probability mass to spread and generalize.</li> </ul> <hr/> <h2 id="9-toy-construction-naive-score-estimator-generalizes-proposition-41">9. Toy construction: Naive score estimator generalizes (Proposition 4.1)</h2> <p>This part constructs an explicit scheme showing how a nontrivial V-kernel arises even if one uses proxy scores directly.</p> <h3 id="91-discretized-pf-ode-sampling-with-euler-steps">9.1 Discretized PF-ODE sampling with Euler steps</h3> <p>Integrate backward with Euler: \(x_{t-\Delta t} = x_t + \Delta t\beta_t x_t + \Delta t D_t (\text{score used at time }t).\)</p> <h3 id="92-at-each-step-sample-an-endpoint-x_0t-from-the-posterior">9.2 At each step sample an endpoint \(x_{0t}\) from the posterior</h3> <p>Sample: \(x_{0t} \sim p(x_0\mid x_t,t) = \frac{p(x_t\mid x_0,t)p_{\text{data}}(x_0)}{p(x_t\mid t)}.\) This captures the ambiguity of which training example could have generated the current noisy point.</p> <h3 id="93-define-the-naive-estimator-used-in-that-step">9.3 Define the naive estimator used in that step</h3> <p>Construct: \(\hat s(x_t,t) := s(x_t,t) + \sqrt{\frac{\kappa}{\Delta t}}\left[\tilde s(x_t,t;x_{0t}) - s(x_t,t)\right].\)</p> <p>Interpretation:</p> <ul> <li>The expected estimator equals \(s\) because \(\mathbb E[\tilde s\mid x,t]=s\).</li> <li>But its variance is nonzero because \(\tilde s\) varies with sampled \(x_{0t}\).</li> </ul> <h3 id="94-plug-into-the-euler-update-explicit-form">9.4 Plug into the Euler update (explicit form)</h3> <p>The paper writes: $$ x_{t-\Delta t} = x_t + \Delta t\beta_t x_t + \Delta t D_t s(x_t,t)</p> <ul> <li>\sqrt{\kappa\Delta t}\, D_t\left[\tilde s(x_t,t;x_{0t})-s(x_t,t)\right]. $$</li> </ul> <p>This is exactly of “drift + sqrt(Delta t) noise” form (Euler–Maruyama).</p> <h3 id="95-derive-s_textavgs">9.5 Derive \(s_{\text{avg}}=s\)</h3> <p>Compute ensemble mean over the fresh sampling of \(x_{0t}\): \([\hat s(x_t,t)] = s(x_t,t) + \sqrt{\frac{\kappa}{\Delta t}}\left[\mathbb E(\tilde s\mid x_t,t)-s\right] = s(x_t,t).\)</p> <p>So the mean drift is the true PF-ODE drift.</p> <h3 id="96-derive-the-v-kernel-for-this-scheme">9.6 Derive the V-kernel for this scheme</h3> <p>Noise at time step is: \(\Delta x_{\text{noise}} = \sqrt{\kappa\Delta t}\, D_t(\tilde s - s).\) So its covariance (conditioned on \(x_t,t\)) is: \(\mathrm{Cov}(\Delta x_{\text{noise}}) = \kappa\Delta t\, D_t \mathrm{Cov}(\tilde s\mid x_t,t) D_t.\) But \(\mathrm{Cov}(\tilde s\mid x_t,t)=C(x_t,t)\) where: \(C(x,t)=S_t^{-1} + \nabla_x^2 \log p(x\mid t).\)</p> <p>If the samples \(x_{0t}\) are independent across time steps, then noise is white in time. In continuous limit this yields: \(V(x_t,t;x_{t'},t') = \kappa D_t C(x_t,t) D_{t'} \delta(t-t').\)</p> <h3 id="97-interpret-the-result-why-this-implies-generalization">9.7 Interpret the result (why this implies generalization)</h3> <ul> <li>Even though this scheme uses proxy score draws tied to training data (so “memorization-like”),</li> <li>the resampling injects noise proportional to proxy-score covariance.</li> <li>Proxy-score covariance is high in boundary regions between training examples. So the effective sampling SDE has more diffusion in those boundary regions, which spreads mass between examples: that is generalization.</li> </ul> <hr/> <h2 id="10-final-qualitative-statement-connect-to-your-last-paragraph">10. Final qualitative statement (connect to your last paragraph)</h2> <p>The effective dynamics:</p> <ul> <li>follow deterministic PF-ODE dynamics <strong>in expectation</strong> because noise mean is zero: \(\mathbb E[\xi]=0.\)</li> <li>and are <strong>most likely</strong> to remain near deterministic PF-ODE paths because large deviations have lower probability under Gaussian noise. Thus the model still samples near training examples most of the time, but with nonzero V-kernel it also places mass in-between.</li> </ul> <p>This precisely matches the paper’s intended notion of “generalize but still prioritize training regions.”</p> <hr/> <h2 id="11-consolidated-answers-to-the-specific-questions-you-raised">11. Consolidated answers to the specific questions you raised</h2> <h3 id="111-why-negative-sign-in-forward-sde">11.1 Why negative sign in forward SDE?</h3> <p>Because it is a mean-reverting drift term ensuring stability and convergence to noise. It does not imply \(x_0=-x_0\); it implies \(\dot x_0\) points toward 0.</p> <h3 id="112-why-marginalization-formula-pxmid-tint-pxmid-x_0tp_textdatax_0dx_0">11.2 Why marginalization formula \(p(x\mid t)=\int p(x\mid x_0,t)p_{\text{data}}(x_0)dx_0\)?</h3> <p>It is the law of total probability: marginalize out the latent clean point \(x_0\).</p> <h3 id="113-why-proxy-score-is-s_t-1alpha_t-x_0-x">11.3 Why proxy score is \(S_t^{-1}(\alpha_t x_0-x)\)?</h3> <p>It is the gradient of the log of a Gaussian density in \(x\).</p> <h3 id="114-why-mathbb-etilde-smid-xts">11.4 Why \(\mathbb E[\tilde s\mid x,t]=s\)?</h3> <p>Differentiate the mixture expression for \(p(x\mid t)\) under the integral sign and use Bayes rule.</p> <h3 id="115-why-covariance-formula-cs_t-1nabla_x2log-p">11.5 Why covariance formula \(C=S_t^{-1}+\nabla_x^2\log p\)?</h3> <p>Compute Hessian of log marginal and express it via posterior expectations; use \(\nabla_x \tilde s=-S_t^{-1}\) for Gaussian conditional.</p> <h3 id="116-why-distribution-of-deterministic-output-is-delta">11.6 Why distribution of deterministic output is delta?</h3> <p>For fixed \(\theta,x_T\) the output is exactly \(F(x_T,\theta)\), so: \(q(x_0\mid x_T;\theta)=\delta(x_0-F(x_T,\theta)).\)</p> <h3 id="117-why-ensemble-average-is-mathbb-e_thetadeltax_0-fx_ttheta-and-not-mathbb-ef">11.7 Why ensemble average is \(\mathbb E_\theta[\delta(x_0-F(x_T,\theta))]\) and not \(\mathbb E[F]\)?</h3> <p>Because we want the full distribution (mass placement), not the mean output. The delta identity defines the induced distribution.</p> <h3 id="118-why-path-integral-helps">11.8 Why path integral helps?</h3> <p>It replaces “solve a complicated ODE” with “integrate over paths constrained to satisfy the ODE,” turning dependence on \(\hat s_\theta\) into a linear functional in an exponent so cumulant methods apply.</p> <h3 id="119-why-if-residual-is-zero-how-can-it-contribute">11.9 Why “if residual is zero, how can it contribute”?</h3> <p>Because integrating over \(p_t\) enforces the constraint via a delta-functional; ODE-consistent paths survive with nonzero weight, inconsistent paths cancel out.</p> <h3 id="1110-why-averaging-mathbb-e_thetaexpint-p_tcdot-d_t-hat-s_theta-dt-is-tractable">11.10 Why averaging \(\mathbb E_\theta[\exp(\int p_t\cdot D_t \hat s_\theta dt)]\) is tractable?</h3> <p>Because it is a characteristic functional; its log expands in cumulants determined by mean and covariance of \(\hat s_\theta\).</p> <hr/> <h2 id="12-generalization-through-variance-consequences-and-example">12 Generalization Through Variance: Consequences and Example</h2> <p>This section discusses the conceptual and practical consequences of generalization through variance.<br/> Rather than introducing new technical machinery, it explains what kind of inductive bias the theory implies, when generalization occurs or does not occur, and how the V-kernel shapes the learned distribution.</p> <hr/> <h3 id="121-benign-properties-of-generalization-through-variance">12.1 Benign Properties of Generalization Through Variance</h3> <p>The theory developed earlier shows that generalization arises through the V-kernel, which is determined by the covariance of the proxy score. Crucially, this covariance has a highly structured form, which strongly constrains how generalization can occur.</p> <hr/> <h4 id="1211-boundary-driven-generalization">12.1.1 Boundary-driven generalization</h4> <p>The key driver of generalization through variance is the proxy score covariance, which is large primarily in boundary regions between training examples. These are regions where the posterior distribution</p> \[p(x_0 \mid x,t)\] <p>is uncertain, meaning that multiple training points could plausibly explain the same noisy observation $x_t$.</p> <p>As a result, generalization through variance:</p> <ul> <li>acts mainly in ambiguous regions between data points,</li> <li>does not create probability mass arbitrarily far from the data,</li> <li>and is therefore strongly constrained.</li> </ul> <p>This boundary-localized behavior is why generalization through variance provides a reasonable inductive bias.</p> <hr/> <h4 id="1212-no-generalization-for-a-single-data-point">12.1.2 No generalization for a single data point</h4> <p>If the dataset contains only one training example (so that $M = 1$), then the posterior $p(x_0 \mid x,t)$ is deterministic. In this case, the proxy score covariance vanishes:</p> \[\mathrm{Cov}_{x_0 \mid x,t}[\tilde s(x,t;x_0)] = 0.\] <p>Since the V-kernel is constructed from this covariance, it follows that</p> \[V(x,t;x',t') = 0.\] <p>Therefore:</p> <ul> <li>the effective dynamics reduce exactly to the deterministic PF-ODE,</li> <li>no stochasticity is introduced,</li> <li>and no generalization occurs.</li> </ul> <p>This shows that generalization requires ambiguity in the data distribution.</p> <hr/> <h4 id="1213-preservation-of-data-manifold-dimensionality">12.1.3 Preservation of data manifold dimensionality</h4> <p>If the data distribution is supported primarily on a low-dimensional data manifold, then uncertainty in $p(x_0 \mid x,t)$ tends to lie along the manifold, not orthogonal to it.</p> <p>Consequently:</p> <ul> <li>the proxy score covariance is nontrivial only along manifold directions,</li> <li>the V-kernel injects noise only in those directions.</li> </ul> <p>Thus, generalization through variance:</p> <ul> <li>preserves the intrinsic dimensionality of the data,</li> <li>rather than spreading probability mass into irrelevant regions of the ambient space.</li> </ul> <hr/> <h4 id="1214-no-extrapolation-far-from-training-data">12.1.4 No extrapolation far from training data</h4> <p>Very far from training examples, the posterior $p(x_0 \mid x,t)$ becomes sharply peaked. In this regime, the proxy score covariance is approximately zero, and therefore the V-kernel vanishes.</p> <p>As a result:</p> <ul> <li>the effective dynamics become deterministic,</li> <li>and no generalization through variance occurs.</li> </ul> <p>This explains why diffusion models do not extrapolate aggressively far beyond the support of the training data.</p> <hr/> <h4 id="1215-why-deterministic-structure-is-largely-preserved">12.1.5 Why deterministic structure is largely preserved</h4> <p>The effective dynamics derived earlier take the form of a stochastic differential equation:</p> \[\dot x_t = -\beta_t x_t - D_t s_{\mathrm{avg}}(x_t,t) + \xi(x_t,t),\] <p>where the noise term satisfies</p> \[\mathbb E[\xi(x_t,t)] = 0.\] <p>Because the noise has zero mean:</p> <ul> <li>the effective PF-ODE follows deterministic PF-ODE dynamics on average.</li> </ul> <p>Moreover, paths that deviate significantly from deterministic PF-ODE trajectories are less likely, since such deviations incur a higher noise cost. Therefore:</p> <ul> <li>the most probable trajectories remain close to deterministic ones.</li> </ul> <p>Although the effective dynamics differ from the deterministic PF-ODE, they do not differ substantially. Regions near training data, which are attractors of the deterministic flow, will still tend to be sampled most frequently.</p> <hr/> <h3 id="122-memorization-and-the-v-kernel-in-the-small-noise-limit">12.2 Memorization and the V-kernel in the Small-Noise Limit</h3> <p>While the effective SDE description explains generalization qualitatively, it does not immediately reveal how the V-kernel reshapes the learned distribution.</p> <p>To gain further insight, the paper considers a small-noise approximation, which is valid when the model is somewhat underparameterized. This corresponds to the regime where</p> \[\kappa = \frac{F}{P} &lt; 1,\] <p>so the V-kernel-induced noise is weak.</p> <hr/> <h4 id="1221-semiclassical-approximation">12.2.1 Semiclassical approximation</h4> <p>When the noise is sufficiently small, the path integral describing the typical learned distribution can be approximated using a semiclassical (saddle-point) approximation.</p> <p>In this limit, the average learned distribution satisfies</p> \[[q(x_0)] \approx p(x_0 \mid \epsilon) \, \frac{1}{\sqrt{ \det\!\left( \frac{1}{\kappa} \frac{\partial^2 S_{\mathrm{cl}}(x_0, x_T^\ast(x_0))} {\partial x_T \partial x_T} \right) }}.\] <p>Here:</p> <ul> <li>$p(x_0 \mid \epsilon)$ is the data distribution convolved with a small amount of diffusion noise,</li> <li>$x_T^\ast(x_0)$ is the most likely noise seed that flows to $x_0$ under deterministic PF-ODE dynamics,</li> <li>$S_{\mathrm{cl}}$ is the action (negative log-likelihood) of the most likely path connecting $x_T^\ast$ to $x_0$.</li> </ul> <hr/> <h4 id="1222-interpretation-of-the-small-noise-limit">12.2.2 Interpretation of the small-noise limit</h4> <p>In words:</p> <ul> <li>the typical learned distribution equals the slightly noised data distribution,</li> <li>multiplied by a curvature factor that measures how sensitive PF-ODE trajectories are to small perturbations.</li> </ul> <p>The V-kernel affects generalization only through this curvature term, by changing the likelihood of small deviations from deterministic PF-ODE dynamics.</p> <p>Although this approximation clarifies the mechanism, the paper notes that it remains difficult to obtain more explicit analytic results beyond this limit.</p> <hr/> <h3 id="123-gap-filling-inductive-bias">12.3 Gap-Filling Inductive Bias</h3> <p>Because the V-kernel is largest in boundary regions between training examples, generalization through variance is often expected to fill in gaps between data points.</p> <p>However, this behavior is not guaranteed in all cases.</p> <hr/> <h4 id="1231-naive-behavior-reduced-boundary-probability">12.3.1 Naive behavior: reduced boundary probability</h4> <p>In naive settings, generalization through variance can actually reduce the probability mass associated with boundary regions. This occurs because increased noise in those regions can cause trajectories to pass through them more quickly.</p> <hr/> <h4 id="1232-role-of-temporal-correlations">12.3.2 Role of temporal correlations</h4> <p>If the model exhibits nontrivial temporal generalization, for example through time-dependent features $\phi(x,t)$, the V-kernel may develop nontrivial temporal autocorrelations.</p> <p>The paper speculates that such temporal structure may allow trajectories to spend more time in boundary regions, enabling gap-filling behavior.</p> <hr/> <h3 id="124-dependence-on-epsilon-and-model-capacity">12.4 Dependence on $\epsilon$ and Model Capacity</h3> <p>The details of generalization depend strongly on two parameters:</p> <ol> <li>The time cutoff $\epsilon$,</li> <li>The ratio $\frac{F}{P}$, which controls whether the model is under- or overparameterized.</li> </ol> <p>Larger values of either parameter increase the magnitude of the V-kernel, leading to larger deviations from deterministic PF-ODE dynamics.</p> <p>In illustrative one-dimensional examples with training points ${-1,0,1}$, the average learned distribution differs from both the true distribution and the deterministic PF-ODE approximation in:</p> <ul> <li>the heights of peaks near training data,</li> <li>and the probability mass between them.</li> </ul> <p>These differences increase as $\epsilon$ and $\frac{F}{P}$ grow, but larger deviation does not necessarily imply better generalization.</p> <hr/> <h3 id="125-featurenoise-alignment">12.5 Feature–Noise Alignment</h3> <p>Finally, the paper emphasizes that generalization depends strongly on the interaction between model features and the proxy score covariance.</p> <p>Different feature sets interact with the V-kernel differently, producing qualitatively different generalization behavior.</p> <p>Experiments show that:</p> <ul> <li>the same data distribution can be generalized in different ways,</li> <li>depending on the orientation of the data,</li> <li>and depending on the feature set used (e.g., Gaussian versus Fourier features).</li> </ul> <p>Thus, generalization through variance is not universal: it is shaped by feature–noise alignment, which determines which gaps are filled and which are not.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="Paper"/><category term="Reading"/><summary type="html"><![CDATA[Paper summary for "Generalization through Variance in Diffusion Models"]]></summary></entry><entry><title type="html">How Diffusion Models Work</title><link href="https://jefferyy-peng.github.io/blog/2025/diffusion-model/" rel="alternate" type="text/html" title="How Diffusion Models Work"/><published>2025-03-19T16:40:16+00:00</published><updated>2025-03-19T16:40:16+00:00</updated><id>https://jefferyy-peng.github.io/blog/2025/diffusion-model</id><content type="html" xml:base="https://jefferyy-peng.github.io/blog/2025/diffusion-model/"><![CDATA[<p>As part of my research into the mechanisms of compositionality in generative AI, I am conducting a review of the foundational literature surrounding Diffusion Models. While these models have achieved ubiquity in text-to-image generation, understanding their underlying mathematical formulations and architectural constraints is essential for addressing their current limitations in factorization and generalization.</p> <p>This post summarizes the theoretical framework of diffusion models, covering the probabilistic definitions, conditioning mechanisms, and the trade-offs between pixel-space and latent-space architectures.</p> <hr/> <h2 id="21-formulations-of-diffusion-models">2.1 Formulations of Diffusion Models</h2> <p>Diffusion models can be formalized through three primary frameworks: <strong>Denoising Diffusion Probabilistic Models (DDPMs)</strong>, <strong>Score-based Generative Models (SGMs)</strong>, and <strong>Stochastic Differential Equations (SDEs)</strong>.</p> <h3 id="211-denoising-diffusion-probabilistic-models-ddpms">2.1.1 Denoising Diffusion Probabilistic Models (DDPMs)</h3> <p>The DDPM framework defines a forward Markov chain that gradually adds Gaussian noise to data \(x_0\) until it approaches an isotropic Gaussian distribution \(x_T\). The transition kernel is defined as: \(q(x_t \mid x_{t-1})=\mathcal{N}\!\left(x_t;\sqrt{1-\beta_t}\,x_{t-1},\beta_t I\right),\) where \(\beta_t \in (0,1)\) controls the noise variance.</p> <p>Let \(\alpha_t := 1-\beta_t\) and \(\bar{\alpha}_t := \prod_{s=1}^{t} \alpha_s\). Then the marginal distribution is:</p> \[q(x_t \mid x_0)=\mathcal{N}\!\left(x_t;\sqrt{\bar{\alpha}_t}\,x_0,(1-\bar{\alpha}_t)I\right).\] <p>Using the reparameterization trick with \(\epsilon \sim \mathcal{N}(0,I)\), we can write:</p> \[x_t=\sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon.\] <p>The generative capability arises from learning a parameterized reverse Markov chain. The model estimates the transition kernel \(p_\theta(x_{t-1}\mid x_t)\) to iteratively denoise the latent variables:</p> \[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\] <p>Training is typically performed by optimizing a variational lower bound (ELBO) on the negative log-likelihood.</p> <blockquote> <p><strong>Step 1</strong>: Marginalize likelihood: the probability of \(x_0\) is obtained by integrating out all latent variables \(x_{1:T}\). This integral is generally intractable.</p> \[\mathbb{E}\big[-\log p_\theta(x_0)\big] = \mathbb{E}\left[-\log \int p_\theta(x_{0:T}) \, dx_{1:T}\right]\] <p><strong>Step 2</strong>: Introduce an auxiliary (variational) distribution \(q(x_{1:T}\mid x_0)\) by multiplying and dividing inside the integral.</p> \[= \mathbb{E}\left[ -\log \int p_\theta(x_{0:T}) \frac{q(x_{0:T})}{q(x_{0:T})} \, dx_{1:T} \right]\] <p><strong>Step 3</strong>: Apply Jensen’s inequality \(-\log\) is convex, moving the log inside the expectation. This produces an upper bound on the negative log-likelihood.</p> \[\le \mathbb{E}_q\left[ -\log \frac{p_\theta(x_{0:T})}{q(x_{1:T}\mid x_0)} \right]\] <p><strong>Step 4</strong>: Next, factorize the model and variational distributions</p> \[p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^{T} p_\theta(x_{t-1}\mid x_t), \quad q(x_{1:T}\mid x_0) = \prod_{t=1}^{T} q(x_t\mid x_{t-1})\] <p><strong>Step 5</strong>: Substituting these factorizations:</p> \[= \mathbb{E}_q\left[ -\log p(x_T)- \sum_{t=1}^{T} \log \frac{p_\theta(x_{t-1}\mid x_t)} {q(x_t\mid x_{t-1})} \right] := \mathcal{L}\] <p>The first term \(\mathbb{E}_q[-\log p(x_T)]\) is fixed and not optimizable, so we optimize the second term:</p> \[\mathbb{E}_{q(x_{t-1}, x_t)} \left[ \log \frac{q(x_t \mid x_{t-1})} {p_\theta(x_{t-1} \mid x_t)} \right] = \mathbb{E}_{q(x_0, x_t)} \Big[ \mathrm{KL}\big( q(x_{t-1} \mid x_t, x_0) \;\|\; p_\theta(x_{t-1} \mid x_t) \big) \Big]+\text{const}\] <p>this is equavalent to a noise-prediction objective, where a neural network \(\epsilon_\theta\) (typically a UNet) minimizes the error between the added noise \(\epsilon\) and the predicted noise:</p> \[L = \mathbb{E} [ \lambda(t)||\epsilon - \epsilon_\theta(x_t, t)||^2 ]\] </blockquote> <h3 id="212-score-based-generative-models-sgms-and-sdes">2.1.2 Score-based Generative Models (SGMs) and SDEs</h3> <p>Alternative formulations focus on the gradient of the log-density of the data instead of noise being added, known as the score function \(\nabla_x \log p(x)\).</p> <ul> <li><strong>SGMs:</strong> In score-based generative modeling, we train a <strong>Noise-Conditioned Score Network (NCSN)</strong> \(s_\theta(x,t)\) to approximate the score (gradient of the log-density) of data corrupted by Gaussian noise. The classical score-matching objective is</li> </ul> \[\frac{1}{2}\,\mathbb{E}_{p_{\text{data}}} \big\| s_\theta(x) - \nabla_x \log p_{\text{data}}(x) \big\|^2 .\] <p>Let \(x_0 \sim q(x_0)\) denote clean data, and define a family of noisy distributions via</p> \[q(x_t \mid x_0) = \mathcal{N}(x_t; x_0, \sigma_t^2 I), \qquad q(x_t) = \int q(x_t \mid x_0) q(x_0)\,dx_0 .\] <p>Conditioning the score network on the noise level \(t\), the score-matching objective becomes (up to a constant factor):</p> \[\mathbb{E}_{t \sim \mathcal{U}[1,T],\, x_0 \sim q(x_0),\, x_t \sim q(x_t \mid x_0)} \left[\lambda(t)\, \big\| s_\theta(x_t, t) - \nabla_x \log q(x_t \mid x_0) \big\|^2 \right].\] <p>The relation between DDPM and SGM is then</p> \[\boxed{ \epsilon_\theta(x,t) = -\,\sigma_t\, s_\theta(x,t) }\] <ul> <li><strong>SDEs:</strong> While DDPMs and SGMs are originally defined using a <strong>finite, discretized noising process</strong>, the diffusion process can be equivalently formulated in <strong>continuous time</strong> as a stochastic differential equation (SDE). This continuous view unifies diffusion models and enables flexible sampling algorithms.</li> </ul> <h4 id="general-forward-diffusion-sde">General Forward Diffusion SDE</h4> <p>The forward noising process is described by the <strong>Score SDE</strong>:</p> \[dx = f(x,t)\,dt + g(t)\,dw,\] <p>where:</p> <ul> <li>\(f(x,t)\) is the <strong>drift (diffusion) term</strong>,</li> <li>\(g(t)\) controls the <strong>noise magnitude</strong>,</li> <li>\(w\) is a standard Wiener process.</li> </ul> <p>This SDE defines a family of marginal distributions \(q_t(x)\) over time.</p> <h4 id="ddpm-as-a-special-case">DDPM as a Special Case</h4> <p>The continuous-time limit of DDPM corresponds to the SDE:</p> \[dx = -\frac{1}{2}\beta(t)\,x\,dt + \sqrt{\beta(t)}\,dw,\] <p>where \(\beta(t)\) is a continuous noise schedule. This formulation mirrors the discrete DDPM noising process in the limit of infinitely many steps.</p> <h4 id="sgm-as-a-special-case">SGM as a Special Case</h4> <p>Score-based Generative Models (SGMs) correspond to the SDE:</p> \[dx = \sqrt{\frac{d[\sigma_t^2]}{dt}}\,dw,\] <p>where \(\sigma(t)\) is the continuous noise scale. Here, the forward process is a pure diffusion without drift.</p> <h4 id="reverse-time-sde-sampling-process">Reverse-Time SDE (Sampling Process)</h4> <p>For any forward SDE of the form above, the <strong>reverse-time SDE</strong> is given by:</p> \[dx = \big[f(x,t) - g(t)^2 \nabla_x \log q_t(x)\big]\,dt + g(t)\,d\bar{w},\] <p>where:</p> <ul> <li>\(\nabla_x \log q_t(x)\) is the <strong>score function</strong>,</li> <li>\(\bar{w}\) is a backward-time Wiener process.</li> </ul> <p>Learning the score function enables generation of samples by simulating this reverse SDE.</p> <h4 id="probability-flow-ode">Probability Flow ODE</h4> <p>In addition to the reverse SDE, there exists an equivalent <strong>deterministic ODE</strong> with identical marginal distributions:</p> \[dx = \left[f(x,t) - \frac{1}{2}g(t)^2 \nabla_x \log q_t(x)\right]\,dt.\] <p>This <strong>probability flow ODE</strong> allows sampling without stochasticity, using standard ODE solvers.</p> <h4 id="sampling-methods">Sampling Methods</h4> <p>Given the learned score function, samples can be generated using:</p> <ul> <li>Reverse-time SDE solvers,</li> <li>Probability flow ODE solvers,</li> <li>Annealed Langevin Dynamics,</li> <li>Predictor–Corrector (PC) samplers combining SDE solvers with MCMC methods (e.g., Langevin MCMC or HMC).</li> </ul> <h2 id="22-conditional-generation-mechanisms">2.2 Conditional Generation Mechanisms</h2> <table> <tbody> <tr> <td>To enable controllable generation (e.g., text-to-image synthesis), diffusion models must incorporate a conditional vector $c$. The reverse process is modified to $p_\theta(x_{t-1}</td> <td>x_t, c)$. Two primary guidance methods dominate the literature:</td> </tr> </tbody> </table> <h3 id="221-classifier-guidance">2.2.1 Classifier Guidance</h3> <p>This approach leverages an auxiliary classifier $p_\phi(c | x_t)$ trained on noisy images. The denoising process can be expressed: \(p_{\theta,\phi}(x_t \mid x_{t+1}, c)=Z \, p_\theta(x_t \mid x_{t+1}) \, p_\phi(c \mid x_t)\) where \(Z\) is a normalization constant. Taking the gradient of the log of the conditional density (ignoring \(Z\)):</p> \[\nabla_{x_t} \log \big( p_\theta(x_t \mid x_{t+1}) \, p_\phi(c \mid x_t) \big)=\nabla_{x_t} \log p_\theta(x_t \mid x_{t+1}) + \nabla_{x_t} \log p_\phi(c \mid x_t).\] <p>Using the relationship between score and noise prediction,</p> \[\epsilon_\theta(x_t, t) = -\sigma_t s_\theta(x_t, t),\] <p>the gradient becomes:</p> <p>\(=-\frac{1}{\sigma_t}\,\epsilon_\theta(x_t, t)+\nabla_{x_t} \log p_\phi(c \mid x_t).\)</p> <blockquote> <p><strong>Key Intuition:</strong> beside the original unconditional score function, the gradient of the classifier is added to guide the denoise process.</p> </blockquote> <h3 id="222-classifier-free-guidance">2.2.2 Classifier-free Guidance</h3> <p>To avoid the computational cost and complexity of training a separate noise-robust classifier, Ho and Salimans (2022) proposed classifier-free guidance. Here, a single diffusion model is trained to handle both conditional and unconditional inputs (where $c = \emptyset$). During sampling, the noise prediction is a linear combination of both outputs, weighted by a scale $w$:</p> \[\tilde{\epsilon}_\theta(x_t, c) = (1 + w)\epsilon_\theta(x_t, c) - w\epsilon_\theta(x_t)\] <p>This method implicitly maximizes the probability of the condition without an external classifier and has become the standard for state-of-the-art models.</p> <blockquote> <p><strong>Key Intuition:</strong> trains one diffusion model to operate in two modes: with condition and without condition. During sampling, the model compares these two predictions. The difference tells you how the condition should push the sample, and scaling that difference lets you control how strongly the generation follows the condition. In effect, the model learns its own “internal classifier gradient” and uses it to guide sampling—without ever training an explicit classifier.</p> </blockquote> <h2 id="23-state-of-the-art-architectures">2.3 State-of-the-Art Architectures</h2> <p>Current text-to-image systems are generally categorized by whether the diffusion process occurs in pixel space or latent space.</p> <h3 id="231-pixel-based-models">2.3.1 Pixel-based Models</h3> <p>These models operate directly on high-dimensional image data.</p> <ul> <li><strong>GLIDE:</strong> Uses classifier-free guidance to generate photorealistic images and demonstrates capabilities in text-guided inpainting [15].</li> <li><strong>Imagen:</strong> A key finding from the development of Imagen is the scaling law regarding text encoders. The authors discovered that increasing the size of the language model (e.g., using T5-XXL) yields greater improvements in image fidelity and image-text alignment than increasing the size of the visual diffusion model itself [16].</li> </ul> <h3 id="232-latent-based-models">2.3.2 Latent-based Models</h3> <p>To address the high computational costs of pixel-space diffusion, <strong>Latent Diffusion Models (LDMs)</strong> utilize an autoencoder to project data into a lower-dimensional latent space [18].</p> <ul> <li><strong>Stable Diffusion:</strong> Applies the diffusion process within this compressed latent space, utilizing cross-attention mechanisms to incorporate text conditioning. This architecture significantly improves inference efficiency [19].</li> <li><strong>DALL-E 2 (unCLIP):</strong> Utilizes the CLIP embedding space. It generates an image embedding from text and then decodes this embedding into an image, leveraging the joint multimodal space learned by CLIP [20].</li> </ul> <h2 id="24-failure-modes-and-limitations">2.4 Failure Modes and Limitations</h2> <p>Despite the fidelity of these models, systematic evaluations reveal persistent limitations in their reasoning capabilities:</p> <ol> <li><strong>Attribute Binding:</strong> Models frequently fail to correctly bind attributes to objects. For example, in a prompt specifying a “red cube” and a “blue cube,” DALL-E 2 may swap the colors or textures [22], [23].</li> <li><strong>Text Rendering:</strong> While semantic understanding is high, the ability to render coherent alphanumeric text remains poor, likely due to tokenization schemes (BPE encoding) that obscure spelling information from the model [23], [24].</li> <li><strong>Physical Consistency:</strong> Generated images often exhibit violations of physical laws, such as incorrect shadow placement or reflections that do not align with the object’s geometry [25].</li> <li><strong>Bias toward Canonical Forms:</strong> When prompted with unusual scenarios (e.g., “a car with triangular wheels”), models often revert to the mean of the training distribution (circular wheels), indicating a lack of true compositional generalization [26].</li> </ol> <p>These failure modes suggest that while diffusion models excel at texture synthesis and semantic association, they struggle with precise factorization and compositional reasoning—a core focus of the subsequent chapters of this thesis [27].</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="Learning"/><summary type="html"><![CDATA[A learning notebook for diffusion models]]></summary></entry></feed>