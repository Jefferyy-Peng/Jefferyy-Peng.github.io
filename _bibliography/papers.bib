---
---

@string{aps = {American Physical Society,}}

@article{xie2024physiolabxr,
  title={Physiolabxr: A python platform for real-time, multi-modal, brain--computer interfaces and extended reality experiments},
  author={Xie, Ziwen and Peng, Yunxiang and Suh, June Pyo and Feiner, Steven and Sajda, Paul and others},
  journal={Journal of Open Source Software},
  volume={9},
  number={93},
  pages={5854},
  preview={joss.05854-5.jpg},
  url={https://joss.theoj.org/papers/10.21105/joss.05854.pdf},
  abbr={JOSS},
  year={2024}
}

@inproceedings{mathere,
  title={" Why Is There a Tumor?": Tell Me the Reason, Show Me the Evidence},
  author={Ma, Mengmeng and Li, Tang and Peng, Yunxiang and Lin, Lu and Beylergil, Volkan and Zhao, Binsheng and Akin, Oguz and Peng, Xi},
  booktitle={Forty-second International Conference on Machine Learning},
  preview={Tumor-ICML.jpg},
  year = {2025},
  selected={true},
  abstract={Medical AI models excel at tumor detection and
segmentation. However, their latent representations often lack explicit ties to clinical semantics,
producing outputs less trusted in clinical practice.
Most of the existing models generate either segmentation masks/labels (localizing where without why) or textual justifications (explaining why
without where), failing to ground clinical concepts
in spatially localized evidence. To bridge this gap,
we propose to develop models that can justify
the segmentation or detection using clinically relevant terms and point to visual evidence. We
address two core challenges: First, we curate a rationale dataset to tackle the lack of paired images,
annotations, and textual rationales for training.
The dataset includes 180K image-mask-rationale
triples with quality evaluated by expert radiologists. Second, we design rationale-informed
optimization that disentangles and localizes finegrained clinical concepts in a self-supervised manner without requiring pixel-level concept annotations. Experiments across medical benchmarks
show our model demonstrates superior performance in segmentation, detection, and beyond.},
  abbr={ICML},
  url={https://openreview.net/pdf?id=r3ZLefVUMO},
}

@unpublished{peng2026circuitgeneralization,
  title = {Inside-Out: Measuring Generalization in Vision Transformers Through Inner Workings},
  author = {Yunxiang Peng, Mengmeng Ma, Ziyu Yao, Xi Peng},
  note = {Under review at CVPR 2026},
  year = {2026},
  preview = {circuit-cvpr26.jpg},
  abbr = {CVPR},
}

@unpublished{peng2026circuitgeneralization,
  title = {Seeing Is Not Believing: Detect and Interpret Cancer Segmentation Failures},
  author = {Mengmeng Ma, Yunxiang Peng, Tang Li, Lu Lin, Binsheng Zhao, Oguz Akin, Xi Peng},
  note = {Under review at CVPR 2026},
  year = {2026},
  preview = {detect-cvpr26.jpg},
  abbr = {CVPR},
}

@unpublished{peng2026circuitgeneralization,
  title = {Learning-Based Synthetic MRI Post-Processing Framework for Automated Contrast Optimization and Brain Segmentation},
  author = {Yunxiang Peng, Jiyo S Athertya, Yajun Ma, Jody Corey-Bloom, Graeme M Bydder, Xi Peng, Jiang Du, Haiying Tang},
  note = {Under review at ISMRM 2026},
  year = {2026},
  abbr = {ISMRM},
}