<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Compositional Generalization in Diffusion Models | Yunxiang Peng </title> <meta name="author" content="Yunxiang Peng"> <meta name="description" content="Papers for " compositional generalization in diffusion models> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jefferyy-peng.github.io/blog/2026/diffusion-compositional-generalization/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yunxiang</span> Peng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Compositional Generalization in Diffusion Models</h1> <p class="post-meta"> Created on January 12, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/diffusion"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion</a>   <a href="/blog/tag/compositional-generalization"> <i class="fa-solid fa-hashtag fa-sm"></i> Compositional-Generalization</a>   ·   <a href="/blog/category/paper-reading"> <i class="fa-solid fa-tag fa-sm"></i> Paper-Reading</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="tldr">TLDR</h2> <ul> <li> <strong>Paper 1 (SIM / Swing-by Dynamics)</strong> is a highly abstract wrapper of compositional generalization as <strong>learning an identity mapping</strong> on a structured Gaussian mixture. It ignores system internals (e.g., diffusion sampling, U-Nets) and develops theory for the learning dynamics of resulting simplified problem.</li> <li> <strong>Paper 2 (CPC/LCS in Conditional Diffusion)</strong> instead goes into <strong>diffusion models directly</strong>, showing that <strong>local inductive bias</strong> (sparse conditional dependencies in the score) is the mechanism that enables compositional / length generalization.</li> </ul> <hr> <h1 id="paper-1--swing-by-dynamics-in-concept-learning-and-compositional-generalization-sim-yang-et-al-iclr-2025">Paper 1 — Swing-by Dynamics in Concept Learning and Compositional Generalization (SIM) (Yang et al., ICLR 2025)</h1> <p><em>PDF: <a href="https://arxiv.org/pdf/2410.08309v3.pdf" rel="external nofollow noopener" target="_blank">2410.08309v3</a></em></p> <h2 id="1-motivation-a-theoretical-wrapper-for-concept-space-diffusion-results">1. Motivation: a theoretical wrapper for “concept space” diffusion results</h2> <p>Prior work evaluates a diffusion model by:</p> <ul> <li>mapping conditioning concepts to a vector,</li> <li>using a classifier to map generated images back to concept accuracy vectors,</li> <li>so a “good generator + classifier system” behaves like an <strong>identity mapping</strong> in concept space.</li> </ul> <p>This paper argues:</p> <blockquote> <p>The salient part is the <em>structured organization</em> of concept space, not the diffusion internals.</p> </blockquote> <p>So they introduce a simplified task: <strong>Structured Identity Mapping (SIM)</strong>.</p> <hr> <h2 id="2-sim-dataset-gaussian-mixture-with-structured-centroids">2. SIM dataset: Gaussian mixture with structured centroids</h2> <p>Let \(d\) be dimension and \(s\le d\) be number of concept classes.</p> <p>Training data consists of \(s\) Gaussian clusters aligned with coordinate axes: \(x_k^{(p)} \sim \mathcal{N}\!\left(\mu_p \mathbf{1}_p,\; \mathrm{diag}(\sigma)^2\right), \quad p\in [s],\; k\in[n].\)</p> <p>Interpretation:</p> <ul> <li>\(\mu_p\) = concept signal strength (cluster mean distance from origin),</li> <li>\(\sigma_p\) = concept diversity (variance along that axis).</li> </ul> <p>The learning problem is identity mapping with MSE: \(L(\theta) = \frac{1}{2sn}\sum_{p=1}^s\sum_{k=1}^n \|f(\theta; x_k^{(p)}) - x_k^{(p)}\|^2.\)</p> <p>Evaluation uses an OOD “composition” point: \(x_b = \sum_{p=1}^s \mu_p \mathbf{1}_p.\)</p> <hr> <h2 id="3-linearization-loss-in-terms-of-covariance">3. Linearization: loss in terms of covariance</h2> <p>Assume the model is linear in input: \(f(\theta; x)=W_\theta x.\)</p> <p>Then (via trace trick) the loss becomes: \(L(\theta) = \frac{1}{2}\|(W_\theta - I)A^{1/2}\|_F^2,\) where the (population) covariance is diagonal: \(A=\mathbb{E}[xx^\top] = \mathrm{diag}(a), \quad a_p= \begin{cases} \sigma_p^2 + \frac{\mu_p^2}{s}, &amp; p\le s,\\ 0, &amp; p&gt;s. \end{cases}\)</p> <p>Key: learning rates along coordinates are controlled by \(a_p\), hence by \(\mu_p\) and \(\sigma_p\).</p> <hr> <h2 id="4-one-layer-linear-model-closed-form-dynamics">4. One-layer linear model: closed-form dynamics</h2> <p>For \(f(W;x)=Wx\) under gradient flow, they derive: \(f(W(t),z)_k = \mathbf{1}_{k\le s}\bigl(1-e^{-a_k t}\bigr)z_k + \sum_{i=1}^s e^{-a_i t} w_{k,i}(0) z_i.\)</p> <p>Interpretation:</p> <ul> <li>A “growth” term drives the correct identity mapping,</li> <li>A “noise” term decays with time and initialization scale.</li> </ul> <p>Consequences:</p> <ul> <li>Concepts with larger \(a_k\) converge faster.</li> <li>Since \(a_k\) increases with \(\mu_k\) and \(\sigma_k\), generalization order is governed jointly by <strong>signal strength</strong> and <strong>diversity</strong>.</li> </ul> <p>This reproduces empirical “concept order” phenomena.</p> <p>Limitation:</p> <ul> <li>coordinates evolve independently → no non-monotonic OOD behavior.</li> </ul> <hr> <h2 id="5-deep-linear-model-symmetric-2-layer-network-and-swing-by-dynamics">5. Deep linear model: symmetric 2-layer network and Swing-by dynamics</h2> <p>They analyze: \(f(U;x)=UU^\top x, \quad W(t)=UU^\top.\)</p> <p>They obtain an evolution equation for Jacobian entries \(w_{i,j}\) decomposed into:</p> <ul> <li>growth term,</li> <li>suppression term,</li> <li>noise term,</li> </ul> <p>which yields <strong>multi-stage dynamics</strong>:</p> <ol> <li>initial growth of many Jacobian entries,</li> <li>one major diagonal entry grows first,</li> <li>it suppresses associated off-diagonal “minor” entries,</li> <li>next major entry grows, and so on.</li> </ol> <p>This staged Jacobian evolution produces an OOD trajectory that:</p> <ul> <li>initially moves toward the OOD composition point,</li> <li>then detours back toward training cluster(s),</li> <li>then later returns to OOD performance.</li> </ul> <p>They call this <strong>Swing-by dynamics</strong> and connect it to a double-descent-like test loss curve (for OOD).</p> <hr> <h2 id="6-empirical-bridge-back-to-diffusion">6. Empirical bridge back to diffusion</h2> <p>Even though SIM is abstract, they verify in text-conditioned diffusion models that:</p> <ul> <li>OOD concept accuracy can be non-monotonic during training,</li> <li>matching the “Swing-by” mechanism predicted by theory.</li> </ul> <hr> <h2 id="7-takeaway-from-paper-2">7. Takeaway from Paper 2</h2> <p>This paper treats compositional generalization as a <strong>wrapper identity mapping problem</strong>:</p> <ul> <li>it ignores internal generative machinery,</li> <li>and explains sequential concept learning + non-monotonic OOD curves as consequences of optimization dynamics on structured data.</li> </ul> <hr> <h1 id="paper-2--local-mechanisms-of-compositional-generalization-in-conditional-diffusion-bradley-et-al-2025">Paper 2 — Local Mechanisms of Compositional Generalization in Conditional Diffusion (Bradley et al., 2025)</h1> <p><em>PDF: <a href="https://arxiv.org/pdf/2509.16447v2.pdf" rel="external nofollow noopener" target="_blank">2509.16447v2</a></em></p> <h2 id="1-motivation-why-length-generalization-is-hard">1. Motivation: why length generalization is hard</h2> <p>The paper studies <strong>length generalization</strong> in conditional diffusion: train on scenes with a small number of objects, then test with <strong>more</strong> conditions (e.g., more specified locations) than seen during training.</p> <p>Key observation:</p> <ul> <li>Whether length generalization succeeds depends on whether the model learns a <strong>compositional mechanism</strong> (one object per condition) or a <strong>shortcut mechanism</strong> (condition triggers a typical scene, not additive per-condition behavior).</li> </ul> <h2 id="2-setup-location-conditioned-clevr-experiments">2. Setup: location-conditioned CLEVR experiments</h2> <p>They use CLEVR with location conditioning:</p> <ul> <li> <strong>Experiment 1 (success):</strong> conditioner labels <strong>all</strong> object locations.</li> <li> <strong>Experiment 2 (failure):</strong> conditioner labels <strong>only one</strong> randomly chosen object location (even if the image has 2–3 objects).</li> <li> <strong>Experiment 3 (fix):</strong> enforce an architecture that induces <strong>local conditional score structure</strong>, restoring length generalization.</li> </ul> <p>Crucial point: Even when training does not include multi-location conditioning, length generalization can still happen if the <strong>right inductive bias</strong> forces the model to represent the conditional distribution compositionally.</p> <hr> <h2 id="3-definitions-score-functions-and-locality">3. Definitions: Score functions and locality</h2> <p>A diffusion model learns the <strong>conditional score</strong>: \(s_t(x \mid c) \;=\; \nabla_x \log p_t(x \mid c).\)</p> <h3 id="31-local-conditional-scores-lcs">3.1 Local Conditional Scores (LCS)</h3> <p>They define <strong>Local Conditional Scores (LCS)</strong> as a sparsity condition on dependencies:</p> <p>At each pixel \(i\), the score depends only on:</p> <ul> <li>a subset of pixels \(N_i\) (often a local neighborhood), and</li> <li>a subset of conditions \(L_i \subseteq J\) (often nearby conditions for location-conditioning).</li> </ul> <p>Informally:</p> <blockquote> <p>The score at pixel \(i\) does not need the entire image nor all conditioners—only a sparse subset.</p> </blockquote> <p>This generalizes “local scores” to conditional settings.</p> <hr> <h2 id="4-conditional-projective-composition-cpc">4. Conditional Projective Composition (CPC)</h2> <p>They define a strong form of compositionality of the <strong>conditional distribution</strong>.</p> <p>Let \(J\) index the set of conditions \(\{c_j\}_{j \in J}\). Let \(\{M_j\}_{j\in J}\) be <strong>disjoint</strong> pixel subsets, and let \(M_J^c\) denote the remaining pixels.</p> <p>A <strong>CPC</strong> distribution factorizes as: \(p(x \mid c_J) \;=\; p(x_{M_J^c} \mid \varnothing) \prod_{j \in J} p(x_{M_j} \mid c_j).\)</p> <p>Meaning:</p> <ul> <li>Each condition \(c_j\) affects only its own region \(M_j\).</li> <li>Different regions are conditionally independent.</li> <li>A background region \(M_J^c\) may be unconditional.</li> </ul> <p>This structure implies length generalization naturally: adding a new condition adds a new independent factor.</p> <hr> <h2 id="5-lemma-1-cpc--lcs-exact">5. Lemma 1: CPC ⇒ LCS (exact)</h2> <p>If \(p(x \mid c_J)\) satisfies CPC, then its score is LCS.</p> <p>Sketch: Take logs: \(\log p(x \mid c_J) = \log p(x_{M_J^c}\mid \varnothing) + \sum_{j\in J}\log p(x_{M_j}\mid c_j).\)</p> <p>Differentiate w.r.t. pixel \(i\):</p> <ul> <li>If \(i \in M_j\), then: \(\nabla_{x_i}\log p(x\mid c_J) = \nabla_{x_i}\log p(x_{M_j}\mid c_j),\) so dependence is only on \(x_{M_j}\) and condition \(c_j\).</li> <li>If \(i \in M_J^c\), then the score depends only on the unconditional background term.</li> </ul> <p>Thus compositional distributions have <strong>local conditional</strong> score structure.</p> <hr> <h2 id="6-relaxation-approximate-cpc--approximate-lcs-and-more-compositional-at-high-noise">6. Relaxation: approximate CPC ⇒ approximate LCS, and “more compositional at high noise”</h2> <p>Real models are not perfectly CPC. The paper relaxes the lemma:</p> <ul> <li>If \(p(x\mid c)\) is <strong>approximately</strong> CPC, then the score is <strong>approximately</strong> LCS.</li> <li>The approximation becomes better at <strong>higher noise</strong> \(t\) (intuitively, noise washes out detailed interactions, leaving large-scale compositional structure).</li> </ul> <p>This supports a diffusion-time decomposition:</p> <ul> <li> <strong>High noise:</strong> conditional dependencies dominate; global structure (object count/layout) is established.</li> <li> <strong>Low noise:</strong> pixel dependencies dominate; local unconditional denoising fills in details.</li> </ul> <p>This explains why local conditional mechanisms can “set” the compositional structure early.</p> <hr> <h2 id="7-feature-space-extension-cpclcs-after-an-invertible-transform">7. Feature-space extension: CPC/LCS after an invertible transform</h2> <p>Pixel-space compositionality often fails for prompts like “watercolor cat sushi” (style and content interact everywhere).</p> <p>They propose: Let \(z = A(x)\) be an invertible feature transform. If the pushforward distribution \(A_\#p(z\mid c)\) is CPC, then the <strong>feature-space score</strong> is LCS.</p> <p>This motivates “disentanglement” as CPC/LCS structure in a learned feature space.</p> <h3 id="71-orthogonality-heuristic-for-disentanglement">7.1 Orthogonality heuristic for disentanglement</h3> <p>Define: \(\mu_i := \mathbb{E}_{z\sim A_\#p(\cdot\mid c_i)}[z], \quad \mu_b := \mathbb{E}_{z\sim A_\#p(\cdot\mid \varnothing)}[z], \quad d_i := \mu_i - \mu_b.\)</p> <p>A necessary (not sufficient) condition for CPC is pairwise orthogonality: \(d_i^\top d_j = 0 \quad \forall i\neq j.\)</p> <p>Practically they compute cosine similarity: \(\frac{d_i^\top d_j}{\|d_i\|\|d_j\|},\) where low off-diagonal similarity suggests feature-space disentanglement.</p> <hr> <h2 id="8-causal-evidence-enforcing-lcs-restores-generalization">8. Causal evidence: enforcing LCS restores generalization</h2> <p>Experiment 3 performs a direct intervention:</p> <ul> <li>keep training distribution like Experiment 2,</li> <li>enforce architectural locality producing LCS-like score dependencies,</li> <li>observe restored length generalization.</li> </ul> <p>Conclusion:</p> <blockquote> <p>The local conditional score structure is not merely correlated with generalization; it is a <strong>causal mechanism</strong>.</p> </blockquote> <hr> <h2 id="9-takeaway-from-paper-1">9. Takeaway from Paper 1</h2> <p>Compositional generalization in conditional diffusion hinges on an <strong>inductive bias</strong>:</p> <ul> <li>representing conditional effects in a sparse / local way (LCS),</li> <li>which corresponds to a compositional factorization of the conditional distribution (CPC).</li> </ul> <hr> <h1 id="synthesis-how-the-two-papers-complement-each-other">Synthesis: How the two papers complement each other</h1> <ul> <li> <strong>Paper 1</strong>: compositionality depends on <em>mechanistic inductive bias in diffusion</em>: local conditional score structure.</li> <li> <strong>Paper 2</strong>: compositional phenomena can arise even in a stripped-down identity task: optimization + data geometry create staged learning and Swing-by.</li> </ul> <p>Together:</p> <ul> <li>Paper 2 explains <em>when</em> and <em>in what order</em> concept directions emerge under training dynamics.</li> <li>Paper 1 explains <em>whether</em> a diffusion system will actually realize an additive compositional mechanism, via locality/sparsity constraints in the conditional score.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-generalization-variance/">Generalization Through Variance in Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-model/">How Diffusion Models Work</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yunxiang Peng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>