<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Generalization Through Variance in Diffusion Models | Yunxiang Peng </title> <meta name="author" content="Yunxiang Peng"> <meta name="description" content="Paper summary for " generalization through variance in diffusion models> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jefferyy-peng.github.io/blog/2025/diffusion-generalization-variance/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yunxiang</span> Peng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Generalization Through Variance in Diffusion Models</h1> <p class="post-meta"> Created on May 21, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Learning</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>”””</p> <p><strong>Formatting rules (per your request):</strong></p> <ul> <li>Every equation (inline or display) is wrapped in \(...\).</li> <li>I use LaTeX backslashes as single characters: I write $\text{like } \alpha_t$ as \(\alpha_t\), <strong>not</strong> \(\\alpha_t\).</li> <li>This writeup follows the <strong>logic of the paper</strong> and explicitly includes/answers the questions we discussed.</li> </ul> <hr> <h2 id="0-high-level-roadmap-what-the-paper-tries-to-do">0. High-level roadmap (what the paper tries to do)</h2> <p>The paper’s main goal is to analytically characterize the <strong>typical learned sampling distribution</strong> of diffusion models trained with denoising score matching (DSM), and to explain <strong>why they generalize</strong> (place probability mass between training examples) instead of perfectly memorizing.</p> <p>Key steps:</p> <ol> <li>Define forward diffusion and reverse PF-ODE sampling.</li> <li>Compare the “true-score” objective \(J_0\) vs DSM objective \(J_1\).</li> <li>Show the proxy score is <strong>unbiased</strong> but has a structured <strong>covariance</strong>.</li> <li>Treat the trained score estimator \(\hat s_\theta\) as random (because it depends on finite training samples).</li> <li>Write PF-ODE sampling as a <strong>path integral</strong>, so that averaging over training randomness becomes tractable.</li> <li>Perform the ensemble average via cumulants, producing mean term \(M_1\) and covariance term \(M_2\) (the <strong>V-kernel</strong>).</li> <li>Under a Gaussian approximation, show the averaged dynamics are equivalent to an <strong>effective SDE</strong>: generalization happens iff the V-kernel is nonzero.</li> <li>Provide a toy “naive estimator” scheme showing how nonzero V-kernel arises even when using the proxy score directly.</li> </ol> <hr> <h2 id="1-preliminaries-distributions-conditionals-and-marginals">1. Preliminaries: distributions, conditionals, and marginals</h2> <h3 id="11-data-distribution">1.1 Data distribution</h3> <p>Let \(x_0 \in \mathbb R^D\) denote clean data. The data distribution is \(p_{\text{data}}(x_0)\).</p> <p>A common idealization used in the paper is an empirical distribution over \(M\) examples \(\{\mu_m\}_{m=1}^M\): \(p_{\text{data}}(x_0) = \frac{1}{M}\sum_{m=1}^M \delta(x_0 - \mu_m).\)</p> <p>Here \(\delta(\cdot)\) is the Dirac delta distribution (we will define it precisely later).</p> <hr> <h2 id="2-forward-diffusion-sde-why-the-minus-sign-and-closed-form-transition">2. Forward diffusion: SDE, why the minus sign, and closed-form transition</h2> <h3 id="21-forward-sde">2.1 Forward SDE</h3> <p>The forward process is an SDE: \(\dot x_t = -\beta_t x_t + G_t \eta_t,\quad t:0\to T.\) Definitions:</p> <ul> <li>\(x_t \in \mathbb R^D\): random variable at time \(t\).</li> <li>\(\dot x_t\): time derivative (informally; in SDE form it corresponds to Ito dynamics).</li> <li>\(\beta_t \ge 0\): scalar drift schedule.</li> <li>\(G_t \in \mathbb R^{D\times K}\): noise injection matrix.</li> <li>\(\eta_t\): standard Gaussian white noise process.</li> </ul> <p>Define diffusion tensor: \(D_t := \frac{G_t G_t^\top}{2}\in \mathbb R^{D\times D}.\)</p> <h4 id="your-question-why-negative-sign-if-t0-then-x_t---x_t">Your question: “why negative sign? if t=0 then x_t = -x_t?”</h4> <p>Important: the equation is about the <strong>derivative</strong>, not equality of values. At \(t=0\):</p> <ul> <li>you set initial condition \(x_0 \sim p_{\text{data}}\).</li> <li>the SDE gives derivative mean drift \(\dot x_0 = -\beta_0 x_0 + \text{noise}\). So it does <strong>not</strong> imply \(x_0 = -x_0\); it implies the drift points toward the origin (mean-reverting), preventing explosion and ensuring the process approaches noise.</li> </ul> <hr> <h3 id="22-solve-the-forward-sde-derive-px_tmid-x_0tmathcal-nalpha_t-x_0-s_t">2.2 Solve the forward SDE: derive \(p(x_t\mid x_0,t)=\mathcal N(\alpha_t x_0, S_t)\)</h3> <p>We derive the conditional distribution of \(x_t\) given \(x_0\).</p> <p>Write the SDE in Ito differential form: \(dx_t = -\beta_t x_t dt + G_t dW_t,\) where \(W_t\) is a \(K\)-dim Brownian motion (so \(dW_t\) are Gaussian increments).</p> <h4 id="step-1-integrating-factor">Step 1: integrating factor</h4> <p>Define \(\alpha_t := \exp\left(-\int_0^t \beta_{t'} dt'\right).\) Note: \(\frac{d}{dt}\alpha_t = -\beta_t \alpha_t,\quad \alpha_0=1.\)</p> <p>Consider the scaled process: \(y_t := \alpha_t^{-1} x_t.\)</p> <p>Use Ito (here drift-only scaling works as usual since factor is deterministic): \(dy_t = d(\alpha_t^{-1} x_t) = \alpha_t^{-1} dx_t + x_t d(\alpha_t^{-1}).\) Compute: \(d(\alpha_t^{-1}) = -\alpha_t^{-2} d\alpha_t = -\alpha_t^{-2}(-\beta_t \alpha_t dt) = \beta_t \alpha_t^{-1} dt.\) So: \(dy_t = \alpha_t^{-1}(-\beta_t x_t dt + G_t dW_t) + x_t(\beta_t \alpha_t^{-1} dt) = \alpha_t^{-1} G_t dW_t.\)</p> <h4 id="step-2-integrate">Step 2: integrate</h4> <p>Integrate from 0 to t: \(y_t = y_0 + \int_0^t \alpha_{t'}^{-1} G_{t'} dW_{t'}.\) But \(y_0 = \alpha_0^{-1} x_0 = x_0\). Thus: \(y_t = x_0 + \int_0^t \alpha_{t'}^{-1} G_{t'} dW_{t'}.\)</p> <p>Multiply by \(\alpha_t\): \(x_t = \alpha_t x_0 + \alpha_t\int_0^t \alpha_{t'}^{-1} G_{t'} dW_{t'}.\)</p> <h4 id="step-3-identify-gaussian-distribution">Step 3: identify Gaussian distribution</h4> <p>The stochastic integral is Gaussian with mean 0. Thus conditional on \(x_0\):</p> <ul> <li>mean: \(\mathbb E[x_t\mid x_0] = \alpha_t x_0.\)</li> <li>covariance: Let \(\varepsilon_t := \alpha_t\int_0^t \alpha_{t'}^{-1} G_{t'} dW_{t'}.\) Then: \(\mathrm{Cov}(\varepsilon_t\mid x_0) = \alpha_t^2 \int_0^t \alpha_{t'}^{-2} G_{t'} \mathrm{Cov}(dW_{t'}) G_{t'}^\top.\) Since \(\mathrm{Cov}(dW_{t'}) = I dt'\): \(\mathrm{Cov}(\varepsilon_t\mid x_0) = \alpha_t^2 \int_0^t \alpha_{t'}^{-2} G_{t'} G_{t'}^\top dt' = \alpha_t^2 \int_0^t \alpha_{t'}^{-2} (2D_{t'}) dt'.\)</li> </ul> <p>Many texts rewrite this as an equivalent expression in terms of the forward-time convention used in the paper: \(S_t := \int_0^t 2D_{t'} \alpha_{t'}^2 dt'.\) (This matches the paper’s definition; it can be obtained by a change of variables depending on whether one defines \(\alpha_t\) relative to 0 or relative to t. The paper uses the above closed form.)</p> <p>Thus: \(p(x\mid x_0,t) = \mathcal N(x; \alpha_t x_0, S_t).\)</p> <hr> <h3 id="23-derive-the-marginal-at-time-t">2.3 Derive the marginal at time \(t\)</h3> <p>Define the marginal: \(p(x\mid t) := \int p(x\mid x_0,t)\, p_{\text{data}}(x_0)\, dx_0.\)</p> <p><strong>Derivation:</strong> law of total probability / marginalization: \(p(x\mid t) = \int p(x,x_0\mid t)\, dx_0 = \int p(x\mid x_0,t)p_{\text{data}}(x_0)\, dx_0.\)</p> <p>If \(p_{\text{data}}\) is discrete mixture of deltas: \(p(x\mid t) = \frac{1}{M}\sum_{m=1}^M \mathcal N(x; \alpha_t \mu_m, S_t).\) So the forward marginal becomes a Gaussian mixture with components centered at scaled training examples.</p> <hr> <h2 id="3-scores-true-score-and-proxy-score-with-full-derivations">3. Scores: true score and proxy score (with full derivations)</h2> <h3 id="31-true-score">3.1 True score</h3> <p>Define the true score: \(s(x,t) := \nabla_x \log p(x\mid t).\)</p> <p>This is the vector field needed by reverse-time sampling methods.</p> <hr> <h3 id="32-proxy-score-dsm-target-and-its-closed-form">3.2 Proxy score (DSM target) and its closed form</h3> <p>Define proxy score: \(\tilde s(x,t;x_0) := \nabla_x \log p(x\mid x_0,t).\) Since: \(p(x\mid x_0,t)=\mathcal N(x; \mu, \Sigma),\quad \mu=\alpha_t x_0,\quad \Sigma=S_t,\) we compute: \(\log \mathcal N(x;\mu,\Sigma) = -\frac{D}{2}\log(2\pi) - \frac{1}{2}\log\det\Sigma - \frac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu).\) Differentiate w.r.t. \(x\): \(\nabla_x \log \mathcal N(x;\mu,\Sigma) = -\frac{1}{2}\nabla_x\left((x-\mu)^\top \Sigma^{-1}(x-\mu)\right).\) Using: \(\nabla_x\left((x-\mu)^\top A (x-\mu)\right)= 2A(x-\mu)\quad \text{for symmetric }A,\) we get: \(\nabla_x \log \mathcal N(x;\mu,\Sigma)= -\Sigma^{-1}(x-\mu) = \Sigma^{-1}(\mu-x).\) Substitute: \(\tilde s(x,t;x_0) = S_t^{-1}(\alpha_t x_0 - x).\)</p> <hr> <h2 id="4-compare-objectives-j_0-vs-j_1-and-why-tilde-s-matters">4. Compare objectives \(J_0\) vs \(J_1\) and why \(\tilde s\) matters</h2> <h3 id="41-define-the-objectives">4.1 Define the objectives</h3> <p>Idealized objective: \(J_0(\theta) := \mathbb E_{t,x}\left[\frac{\lambda_t}{2}\|\hat s_\theta(x,t) - s(x,t)\|^2\right].\)</p> <p>DSM objective: \(J_1(\theta) := \mathbb E_{t,x_0,x}\left[\frac{\lambda_t}{2}\|\hat s_\theta(x,t) - \tilde s(x,t;x_0)\|^2\right].\)</p> <p>Here the sampling is:</p> <ul> <li> \[t\sim p(t)\] </li> <li> \[x_0\sim p_{\text{data}}(x_0)\] </li> <li> \[x\sim p(x\mid x_0,t)\] </li> </ul> <hr> <h3 id="42-unbiasedness-derive-mathbb-e_x_0mid-xttilde-ss">4.2 Unbiasedness: derive \(\mathbb E_{x_0\mid x,t}[\tilde s]=s\)</h3> <p>We prove: \(\mathbb E_{x_0\mid x,t}[\tilde s(x,t;x_0)] = s(x,t).\)</p> <p>Start from definition: \(p(x\mid t)=\int p(x\mid x_0,t)p_{\text{data}}(x_0)\, dx_0.\) Differentiate w.r.t. \(x\): $$ \nabla_x p(x\mid t) = \int \nab_</p> <p>”””</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-model/">How Diffusion Models Work</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yunxiang Peng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>