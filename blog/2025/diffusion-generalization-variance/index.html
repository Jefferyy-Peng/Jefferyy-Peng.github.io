<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Generalization Through Variance in Diffusion Models | Yunxiang Peng </title> <meta name="author" content="Yunxiang Peng"> <meta name="description" content="Paper summary for " generalization through variance in diffusion models> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jefferyy-peng.github.io/blog/2025/diffusion-generalization-variance/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yunxiang</span> Peng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Generalization Through Variance in Diffusion Models</h1> <p class="post-meta"> Created on May 21, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Learning</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>”””</p> <h1 id="generalization-through-variance-in-diffusion-models--logical-summary">Generalization Through Variance in Diffusion Models — Logical Summary</h1> <h2 id="1-core-question-of-the-paper">1. Core Question of the Paper</h2> <p>The paper asks a precise and nonstandard question:</p> <blockquote> <p><strong>Why do diffusion models trained with denoising score matching (DSM) generate samples that go <em>beyond</em> memorizing the training data, even when the true score would only reproduce the training examples?</strong></p> </blockquote> <p>Equivalently:</p> <ul> <li>Why does training with the <em>proxy score</em> \(\tilde s(x,t;x_0)\) lead to <strong>generalization</strong>, whereas the <em>true score</em> \(s(x,t)\) would lead to exact memorization?</li> </ul> <p>The paper’s answer is:</p> <blockquote> <p><strong>Generalization arises from variance in the learned score, not from bias.</strong></p> </blockquote> <hr> <h2 id="2-forward-diffusion-and-scores-setup">2. Forward Diffusion and Scores (Setup)</h2> <h3 id="forward-process">Forward process</h3> <p>Data points \(x_0 \sim p_{\text{data}}\) are corrupted by a linear SDE: \(\dot x_t = -\beta_t x_t + G_t \eta_t,\) leading to Gaussian conditionals: \(p(x \mid x_0, t) = \mathcal N(x; \alpha_t x_0, S_t).\)</p> <h3 id="scores">Scores</h3> <ul> <li> <p><strong>True score</strong>: \(s(x,t) := \nabla_x \log p(x \mid t).\)</p> </li> <li> <p><strong>Proxy score</strong> (DSM target): \(\tilde s(x,t;x_0) := \nabla_x \log p(x \mid x_0,t) = S_t^{-1}(\alpha_t x_0 - x).\)</p> </li> </ul> <p>Key identity: \(\mathbb E_{x_0 \mid x,t}[\tilde s(x,t;x_0)] = s(x,t).\)</p> <p>So the proxy score is <strong>unbiased</strong>, but <strong>noisy</strong>.</p> <hr> <h2 id="3-dsm-objectives-and-the-key-observation">3. DSM Objectives and the Key Observation</h2> <p>Two losses:</p> <ul> <li>Ideal (unavailable): \(J_0 = \mathbb E\, \| \hat s - s \|^2\)</li> <li>Practical (DSM): \(J_1 = \mathbb E\, \| \hat s - \tilde s \|^2\)</li> </ul> <p>Although \(J_0\) and \(J_1\) share the same minimizer in infinite-capacity limits, <strong>models trained with \(J_1\) generalize better</strong>.</p> <p>The paper shows this is because the proxy score has <strong>nontrivial covariance</strong>: \(C(x,t) := \mathrm{Cov}_{x_0 \mid x,t}[\tilde s(x,t;x_0)] = S_t^{-1} + \nabla_x^2 \log p(x \mid t).\)</p> <p>This covariance is:</p> <ul> <li>large at small \(t\) (since \(S_t \to 0\)),</li> <li>large near training points and between them (high curvature regions).</li> </ul> <hr> <h2 id="4-sampling-via-pf-ode-and-the-central-difficulty">4. Sampling via PF-ODE and the Central Difficulty</h2> <p>Sampling uses the <strong>probability flow ODE (PF-ODE)</strong>: \(\dot x_t = -\beta_t x_t - D_t \hat s_\theta(x_t,t).\)</p> <p>For fixed \(\theta\) and \(x_T\), this is <strong>deterministic</strong>: \(x_0 = F(x_T, \theta).\)</p> <p>But:</p> <ul> <li>\(\hat s_\theta\) depends on random training samples,</li> <li>hence \(F(x_T,\theta)\) is a <strong>random variable</strong>.</li> </ul> <p>We want the <strong>distribution</strong> of outputs, not their mean: \(\bar q(x_0 \mid x_T) = \mathbb E_\theta[\delta(x_0 - F(x_T,\theta))].\)</p> <p>This delta form is <strong>not optional</strong> — it is the definition of the distribution induced by a random variable.</p> <hr> <h2 id="5-path-integral-reformulation-key-technical-contribution">5. Path Integral Reformulation (Key Technical Contribution)</h2> <p>Directly averaging \(F(x_T,\theta)\) is intractable because:</p> <ul> <li>PF-ODE has no closed-form solution,</li> <li>\(\hat s_\theta\) enters nonlinearly through the trajectory.</li> </ul> <p><strong>Key trick</strong>: Rewrite PF-ODE sampling as a <strong>path integral</strong> enforcing the ODE as a constraint: \(q(x_0 \mid x_T; \theta) = \int \mathcal D[x_t]\mathcal D[p_t]\, \exp\left( \int_T^\epsilon i p_t \cdot [\dot x_t + \beta_t x_t + D_t \hat s_\theta(x_t,t)]\,dt \right).\)</p> <ul> <li>The auxiliary field \(p_t\) enforces the ODE via a delta-functional.</li> <li>Crucially, \(\hat s_\theta\) appears <strong>linearly inside the exponent</strong>.</li> </ul> <p>This transforms the problem into averaging an <strong>exponential of a linear functional of a random function</strong>.</p> <hr> <h2 id="6-averaging-over-training-randomness">6. Averaging Over Training Randomness</h2> <p>Because of linearity, the ensemble average becomes a <strong>cumulant expansion</strong>: \([ q(x_0 \mid x_T) ] = \int \mathcal D[x_t]\mathcal D[p_t]\, \exp\left( M_1 - \tfrac12 M_2 + \cdots \right).\)</p> <p>Where:</p> <ul> <li>\(M_1\) depends on the <strong>mean score</strong> \(s_{\text{avg}}(x,t) := [\hat s_\theta(x,t)].\)</li> <li>\(M_2\) depends on the <strong>score covariance</strong> \(V(x,t;x',t') := D_t\,\mathrm{Cov}_\theta[\hat s(x,t),\hat s(x',t')]\,D_{t'}.\)</li> </ul> <p>Assuming higher cumulants are negligible (Gaussian approximation), the averaged dynamics are equivalent to an SDE.</p> <hr> <h2 id="7-main-result-effective-sde-for-the-typical-learned-model">7. Main Result: Effective SDE for the Typical Learned Model</h2> <p><strong>Proposition (central result)</strong>: Sampling from \([q(x_0 \mid x_T)]\) is equivalent to integrating: \(\dot x_t = -\beta_t x_t - D_t s_{\text{avg}}(x_t,t) + \xi(x_t,t),\)</p> <p>where: \(\mathbb E[\xi] = 0, \qquad \mathbb E[\xi(t)\xi(t')] = V(x_t,t;x_{t'},t').\)</p> <p>Interpretation:</p> <ul> <li>The <strong>mean dynamics</strong> follow the deterministic PF-ODE.</li> <li> <strong>Generalization arises entirely from the noise term</strong>.</li> </ul> <p>If \(V = 0\) → pure memorization. If \(V \neq 0\) → stochastic spreading → generalization.</p> <hr> <h2 id="8-concrete-example-naive-score-estimator">8. Concrete Example: Naive Score Estimator</h2> <p>The paper shows this is not abstract theory by constructing a toy algorithm:</p> <p>At each Euler step:</p> <ol> <li>Sample \(x_{0t} \sim p(x_0 \mid x_t,t)\),</li> <li>Use a noisy score: \(\hat s(x_t,t) = s(x_t,t) + \sqrt{\kappa/\Delta t} \,[\tilde s(x_t,t;x_{0t}) - s(x_t,t)].\)</li> </ol> <p>Despite using the proxy score directly (which looks like memorization), the <strong>resampling-induced variance</strong> yields: \(V(x,t;x',t') = \kappa D_t C(x,t) D_{t'}\,\delta(t-t').\)</p> <p>Thus:</p> <ul> <li>Noise is strongest where proxy-score covariance \(C(x,t)\) is large,</li> <li>i.e. near boundaries between training examples.</li> </ul> <p>This proves:</p> <blockquote> <p><strong>Variance alone is sufficient to induce generalization.</strong></p> </blockquote> <hr> <h2 id="9-final-interpretation">9. Final Interpretation</h2> <ul> <li>The effective PF-ODE: <ul> <li>follows deterministic score flow <strong>on average</strong>,</li> <li>most probable trajectories stay near deterministic paths,</li> <li>but structured noise spreads probability mass.</li> </ul> </li> </ul> <p>Therefore:</p> <ul> <li>Training examples remain dominant modes,</li> <li>but probability mass fills in between them.</li> </ul> <p>This explains <strong>why diffusion models generalize without forgetting</strong>.</p> <hr> <h2 id="10-one-sentence-takeaway">10. One-Sentence Takeaway</h2> <blockquote> <p><strong>Diffusion models generalize because randomness in score estimation induces an effective stochastic dynamics whose noise is strongest exactly where the model is uncertain, causing probability mass to spread between training examples while preserving their dominance.</strong> “””</p> </blockquote> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-model/">How Diffusion Models Work</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yunxiang Peng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>